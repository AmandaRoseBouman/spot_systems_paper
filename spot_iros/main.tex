%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\documentclass[letterpaper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{graphicx}
\graphicspath{ {./graphics/} }

\usepackage{xcolor}
\usepackage{subcaption}
\captionsetup{font=footnotesize}

\newcommand{\ph}[1]{{\textbf{#1}:}} % paragraph header
\newcommand{\todo}[1]{{\color{red} #1 }} % Tasks to do
\newcommand{\inst}[1]{{\color{orange} #1 }} % instructions
% \newcommand{\kyon}[1]{{\color{blue} #1 }}
\newcommand{\ali}[1]{{\color{orange}A:#1}} % Tasks to do --Previously BLUE
\newcommand{\rev}[1]{{\color{blue}#1}} % Tasks to do --Previously BLUE
\newcommand{\comment}[1]{} 

\usepackage[normalem]{ulem}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{textcomp}

\title{\LARGE \bf
Autonomous Spot: \\
Long-range Autonomous Exploration of \\Extreme Environments with Legged Locomotion
%NeBula Autonomy on Boston Dynamics \textsc{Spot} Robot: \\
%Enabling Autonomous Exploration of Extreme Environments% Title consideration:
% 1. People can easily search "Autonomous Spot" and the paper will appear
% 2. A 2-4 noun words to recall: like Autnomous Spot in Challenging environment
}


\author{Amanda Bouman$^{*2}$, Muhammad Fadhil Ginting$^{*1}$, Nikhilesh Alatur$^{*2}$, Matteo Palieri$^{1}$, \\
David D. Fan$^{1}$, Thomas Touma$^{2}$, Torkom Pailevanian$^{1}$, Sung-Kyun Kim$^{1}$, Kyohei Otsu$^{1}$,\\
Joel Burdick$^{2}$, Ali-akbar Agha-Mohammadi$^{1}$% <-this % stops a space
\thanks{$^*$ These authors contributed equally to this work.}
\thanks{$^{1}$ NASA Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA,
     {\tt\small aliagha@jpl.nasa.gov}}%
\thanks{$^{2}$Department of Mechanical and Civil Engineering, Division of Engineering and Applied Science, California Institute of Technology, Pasadena, CA, USA,
         {\tt\small jwb@robotics.caltech.edu}}%
% \thanks{$^{3}$ Department of Electrical And Information Engineering, Polytechnic University of Bari, Bari, Italy}%
% \thanks{$^{4}$Department of Mechanical and Process Engineering, Institute of Robotics and Intelligent Systems, Swiss Federal Institute of Technology in Zurich, Zurich, Switzerland}%
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% This paper serves as one of the first efforts to enable large-scale and long-term autonomy using the Boston Dynamics Spot robot. 
% Motivated by exploring extreme \rev{environments, particularly those involved in the DARPA Subterranean Challenge,} this paper pushes the boundaries of the state-of-practice in enabling legged robotic systems to accomplish real-world complex missions in relevant scenarios. 
% In particular, we will discuss the behaviors and capabilities which emerge from the integration of the autonomy architecture NeBula (Networked Belief-aware Perceptual Autonomy) with next-generation mobility systems.
% We will discuss the hardware and software challenges, and solutions in mobility, perception, autonomy, and very briefly, wireless networking, as well as lessons learned and future directions. 
% We demonstrate the performance of the proposed solutions on physical systems in real-world scenarios.
% The proposed solution contributed to winning 1st-place in the 2020 DARPA Subterranean Challenge, Urban Circuit.
% \end{abstract}
\begin{abstract}
This paper serves as one of the first efforts to enable large-scale and long-term autonomy using the Boston Dynamics Spot robot. Motivated by exploring extreme \rev{environments, particularly those involved in the DARPA Subterranean Challenge,} this paper pushes the boundaries of the state-of-practice in enabling legged robotic systems to accomplish real-world complex missions in relevant scenarios. 
In particular, we discuss the behaviors and capabilities which emerge from the integration of the autonomy architecture NeBula (Networked Belief-aware Perceptual Autonomy) with next-generation mobility systems.
We will discuss the hardware and software challenges, and solutions in mobility, perception, autonomy, and very briefly, wireless networking, as well as lessons learned and future directions. 
We demonstrate the performance of the proposed solutions on physical systems in real-world scenarios.
The proposed solution contributed to winning 1st-place in the 2020 DARPA Subterranean Challenge, Urban Circuit.
\end{abstract}
%Video: Experimental results are available at ... 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Instructions}
% \ph{Paragraph header} 
% \inst{Please start \uline{every single paragraph} with a paragraph header, summarizing the intention of that paragraph. This is mainly for iterations during the paper preparation. We can remove most of them for the final report.

% Graphs are available here for modification : 
% \url{https://docs.google.com/presentation/d/1dja02gUoBeNuNW-aHb6CmwaFevmf-jXMHk02NYbL1Ok/edit?usp=sharing}
% }
% {\color{red}fadnik means Fadhil or Nikhilesh}

% General Flow of the paper
% Introduction
% Architecture -> philosopy NeBula
% Mobility manager -> locomotion include from BD coauthor from them and sensors
% State Estimation -> Spot Frontend
% Perception Aware Planning -> Perception Aware Planning to Face Forward ..
% Experimental Result
% Lesson Learned  and Future:
% Recovery,
% Maybe stairs

% Conclusions



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \st{\rev{Autonomous} robot and autonomous mapping and traversal of extreme environments under time constraints has a wide variety of real-world applications\rev{, including} . They include search and rescue after natural disasters \cite{Nagatani2013}, exploration of extreme planetary terrains \cite{ali-agu}, \cite{Taka2020JFR}, and \cite{Husain2013} inspection of urban underground infrastructure in urban environments \cite{Kolvenbach2020} and more.
% %\rev{This paper discusses Team CoSTAR's synergistic approach of combining advanced quadrupedal locomotion and high-level autonomy in order to accomplish complex missions.}
% %In this paper, we discuss how bridging advanced locomotion capabilities and autonomy can address such complex missions. 
% %In this paper, we discuss how bridging advanced locomotion capabilities and autonomy can address such complex missions.
% To that end, this paper discusses Team CoSTAR's research and development and synergistic approach of endowing a sophisticated legged platform with high-level autonomy in order to accomplish complex missions. %We focus our research and development around real-world robotics scenarios. 
% As such, the DARPA Subterranean (SubT) Challenge \cite{subt_webpage} presented us with the ideal setting to demonstrate the efficacy of our entire system architecture. 
% The SubT challenge requires robot teams to rapidly explore, map, and search extreme underground environments in order to find objects of interest.}

\rev{Autonomous} robot \st{and autonomous} mapping and traversal of extreme environments under time constraints has a wide variety of real-world applications\rev{, including} \st{. They include} search and rescue after natural disasters \cite{Nagatani2013}, exploration of extreme planetary terrains \cite{ali-agu}, \cite{Taka2020JFR}, \st{and} \cite{Husain2013}, \rev{and} inspection of urban underground \st{infrastructure in urban} environments \cite{Kolvenbach2020} \st{and more}.
%\rev{This paper discusses Team CoSTAR's synergistic approach of combining advanced quadrupedal locomotion and high-level autonomy in order to accomplish complex missions.}
%In this paper, we discuss how bridging advanced locomotion capabilities and autonomy can address such complex missions. 
%In this paper, we discuss how bridging advanced locomotion capabilities and autonomy can address such complex missions.
\st{To that end, this} \rev{This} paper discusses Team CoSTAR's \st{research and development and synergistic approach of endowing} \rev{development and implementation of a hardware-integrated framework which endows} a sophisticated legged platform with high-level autonomy in order to \st{accomplish} \rev{execute} complex missions. %We focus our research and development around real-world robotics scenarios. 
\st{As such, the DARPA Subterranean (SubT) Challenge} \cite{subt_webpage} \st{presented us with the ideal setting to demonstrate the efficacy of our entire system architecture.}

\rev{We verified the efficacy of our hardware-integrated architecture in the 2020 DARPA Subterranean (SubT) Challenge, Urban Circuit \cite{subt_webpage}. Our SubT objective was to guide a team of robots to rapidly navigate and map an extreme underground urban environment with the goal of locating certain artifacts.


}
The SubT challenge requires robot teams to rapidly explore, map, and search extreme underground environments in order to find objects of interest.
%\st{As a concrete mission,} we focus on the DARPA Subterranean (SubT) Challenge \cite{subt_webpage}, a robotic competition that targets one such mission to explore, map, and search extreme underground environments. 
%\rev{The complexity of these missions presents mobility challenges which impose constraints on the solution space.}
%From a mobility perspective, this mission imposes lots of constraints on the solution space.
%\rev{Various requirements imposed by elements of extreme environments include:}


% %1) ability to carry a large-enough payload that can enable high-levels of sensing, autonomy, computing, and communication capabilities, 2) endurance for long-term missions (at least 1 hour) and large areal coverage (multi-kilometers in length), 3) to be small enough to go through passages as narrow as 80 cm in diameter, and 4) ability to traverse mobility-stressing features such as stairs, uneven terrains and obstacle-laden areas with high-risk regions. 

% Elements of these extreme environments %dictate requirements that 
% necessitate novel mobility capabilities development. These requirements include:
% %Some conflicting requirements imposed by elements of such extreme environments are
% 1) sufficient payload capacity for sensing, autonomy, computation, communication and mechanisms, 2) endurance for long-duration, complex and spatially large (\texttildelow $km^2$) mission scenarios, 3) system dimensions for traversing anthropomorphically analogous passages ($\geq$80 cm), 4) advanced traversability capabilities for staircases, uneven terrain, obstacle laden surfaces and high-risk regions.}

% Elements of these extreme environments %dictate requirements that 
% necessitate novel mobility capabilities development. These requirements include:
% %Some conflicting requirements imposed by elements of such extreme environments are
% 1) sufficient payload capacity for sensing, autonomy, computation, communication and mechanisms, 2) endurance for long-duration, complex and spatially large (\texttildelow $km^2$) mission scenarios, 3) system dimensions for traversing anthropomorphically analogous passages ($\geq$80 cm), 4) advanced traversability capabilities for staircases, uneven terrain, obstacle laden surfaces and high-risk regions.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.5\textwidth]{graphics/spot_cover_ver1.jpg}
  \caption{%This picture shows 
  \rev{Live-mission snapshot of Autonomous Spot} (Au-Spot [/awe-spot/]) robot climbing down four flights of stairs in the Urban Circuit of the DARPA Subterranean Challenge. Au-Spot, in an autonomous, collaborative effort with other robots in our team (CoSTAR) \rev{earned us} the 1st-place prize (Image credit: DARPA).} %You could also say something like "Au-Spot, in an autonomous, collaborative effort with our other robots propelled our team (CoSTAR) to a 1st-place finish (Image credit: DARPA).
  
  %This robot, along with Team CoSTAR's other robotic platforms won this challenge (Image credit: DARPA).
  
  \label{fig:stairs-firstPage}
\end{figure}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/spot_sa_v5.pdf}
  \caption{Overview of the system architecture \rev{that powers}NeBula and enables high-level autonomy on Spot. \rev{Red paths denote NeBula's belief-aware planning where the planner aims to minimize mission risk by steering the inference.} 
  %The red arrow highlight the belief-aware planning of NeBula where the planner try to stir the inference system to minimize mission risk.
}
  \label{fig:spot_sysarch}
\end{figure*}

\ph{Legged Mobility}
Legged robots offer unique mobility capabilities which makes them highly suitable to traverse challenging environments \rev{that} would prove difficult for wheeled robots.
%that can adapt to various challenging environments. 
They have the ability to meet locomotion, payload, size, and endurance requirements to operate in extreme environments. %of the \rev{DARPA}missions. 
ANYmal~\cite{whyrobotdeepmines,Bellicoso2018}, Robosimian~\cite{Karumanchi2017}, DRC-HUBO+~\cite{jung2018development}, Nimbro Momaro~\cite{schwarz2017nimbro}, MIT Cheetah~\cite{mit_cheetah}, BigDog~\cite{bigdog}, Ghost Robotics \rev{Vision 60}~\cite{miller2019tunnel} are examples of %such 
\rev{prominent and novel legged} %powerful 
systems. 
%However, the research community is in the just at the beginning of the road to enable these machines to autonomously carry out complex and full missions in challenging real-life environments \cite{Delmerico2019}.

The robotics research community is now in the early stages of empowering legged robots with high levels of autonomy to carry out complex missions in challenging, real-life environments~\cite{Delmerico2019}.

Ramezani et al.~\cite{Ramezani2020} equipped the ANYmal quadruped with a LIDAR SLAM framework for autonomous mapping capabilities. Their particular solution requires manual teleoperation to build an initial map of the environment, upon which the robot can \rev{only} then autonomously navigate within that map and react to previously unidentified obstacles.
%%%Tom: It should make it clear with the "only" that their solution is limited, implicitly comparing it to our solution which is not limited. I feel like this is a non-rude way of saying it. 
%employ its global planner and perform autonomous exploration within the manually attained map. 
The researchers then successfully demonstrated autonomous point-to-point navigation in an industrial complex.
%In \cite{Ramezani2020}, the authors first generate a global map by teleoperating an ANYmal quadruped which has been equipped with LiDAR SLAM.Subsequently, a global planner is employed to navigate the quadruped to any goal point within the constructed map, which even re-plans in the case of newly appeared obstacles.
%They successfully demonstrated autonomous point-to-point navigation in an industrial complex containing stairs.
%In \cite{Ramezani2020}, the authors describe their system which consists of an ANYmal quadruped running LiDAR SLAM. %8
%While they require teleoperation to build an initial map of the environment, from then on they can send the robot to any given goal within that map, considering terrain traversability. %4, better English, write critique more positive

Bayer et al.~\cite{Bayer2019} demonstrate fully autonomous exploration in rough, single-level, indoor and outdoor terrains.
%in- and outdoors.
The researchers augmented an experimental hexapod platform with commercial vision sensors which they used for localization and terrain mapping.
%As experimental platform, they used a hexapod which has been equipped with commercial vision sensors for localization and terrain mapping.
%Bayer et al. \cite{Bayer2019}, demonstrate the capabilities of small hexapods equipped with low-cost commercial vision sensors, which are used for localization and mapping.
%Their hexapod is shown autonomously exploring rough terrain in-% and outdoors within a single level. %4, more positive  (p2)
%Recently, Miller et al. \cite{Miller2019} proposed the first work that goes towards full autonomy with quadrupeds in GPS-denied environments. Their Ghost Vision 60 was equipped with a high level of autonomy to explore tunnel-like environments during the DARPA Subterranean Challenge, Tunnel Circuit.
%Impressive results have been shown in  Miller et al. \cite{Miller2019}, who paved the way towards fully autonomous exploration of unknown, GPS-denied, subterranean environments with legged robots. The authors adopted a Ghost Vision 60 with a high-level of autonomy to explore tunnel-like environments during the 2019 DARPA Subterranean Challenge, Tunnel circuit.

Miller et al.~\cite{miller2019tunnel} 
%have demonstrated impressive results with their legged platform. They 
endowed a Ghost Vision 60 quadruped with higher levels of autonomy to explore a tunnel environment during the 2019 DARPA Subterranean Challenge, Tunnel Circuit. To our knowledge, they were one of the first researchers to demonstrate  
%paved the way towards 
autonomous legged exploration in unknown, GPS-denied, subterranean environments. Their solution was %ideal for 
\rev{shaped around} single-level, tunnel-focused autonomy. %%%Tom: This is a nice way of saying they have an over-fitted solution for tunnels.  
% \todo{fadil} \ph{Quadrupet real world applicationt: Anymal, Ghost, and Spot!}
% \inst{we should say great and positive things about Ghost and Anymal. and mention the state of practice in their autonomy...
% It should NOT be a "comparative paper". Instead of saying we are better, say that this is the "first autonomous behaviors in this scale on Spot". Compare the current results with the past results for Spot (rather than comparing with other platforms).}
% While various legged mobility solutions have been developed recently, 
% % Fadhil: cite 2-3 legged robot platform papers, 
% they have been minimally utilized  in  real-world  autonomous  applications\cite{whyrobotdeepmines}. 
% %
% One reported solution is an application of quadrupedal robot ANYmal in industrial inspection and search and rescue\cite{Bellicoso2018}.
% %
% In DARPA SubT Challenge, PLUTO team deployed multiple Ghost Robotics (GR) Vision 60 quadrupeds autonomously.\cite{miller2019tunnel}
% %
% While in industry world, Boston Dynamics' Spot, a direct successor to Big Dog\cite{bigdog}, shows impressive result in various applications(cite Spot web pages).
% %
% Recently, Boston Dynamic starts leasing their Spots to various industry and research communities to develop Spot application in real-world problems.

% % -Anymal: Anymal in real world application, Anymal‐toward legged robots for harsh environments. Advanced Robotics\cite{hutter2017anymal} 
% % - Ghost robotics: Upenn's Mine  Tunnel  Exploration  using  Multiple  Quadrupedal  Robots.

% % % another example
% % publicized work in 2008 . Recently they start collaborating.
% % How we expand Spot capability in extreme environment
% % emphasize our pioneer work in Spot autonomy

\ph{Contributions} %{Scope}
In this work, we focus on Boston Dynamics' Spot robot \rev{as our base mobility platform} (Fig. \ref{fig:stairs-firstPage}). 
We briefly discuss the NeBula (Networked Belief-aware Perceptual Autonomy) architecture and explain some elements of integrating comprehensive autonomy %on 
\rev{with} the Spot robot.
We describe the induced behaviors and performance of the system in a complex, autonomous mission during the Urban Circuit of the DARPA Subterranean Challenge.
%\rev{in which our team took 1st place at the Urban Circuit of the}
%led to the first place at the Urban Circuit of 
%DARPA Subterranean (SubT) Challenge. 
While the main objective of this paper is to provide a system-level overview of the entire autonomy stack and robot locomotion, we will describe in further detail some specific aspects of the algorithms that are critical to enabling legged autonomy in complex missions. 
%In particular, a few highlights 

Highlights of this paper or areas where we advance the current state-of-practice on Spot and legged robots are:
\begin{enumerate}
    \item Endowing a legged platform with high-level autonomy so that it may traverse %Enabling a legged platform to traverse
    distances greater than 1km in a multi-level, underground, GPS-denied environment within 60 minutes. 
    %with high-levels of autonomy. 
    %Demonstrating he performance at the Urban Circuit of the DARPA Subterranean Challenge.
    \item Enabling reliable multi-sensor odometry in perceptually-degraded environments.
    \item Demonstrating perception- and traversability-aware local planning on legged platforms to negotiate challenging terrains 
    %and limited sensing capabilities.
    and perceptually-degraded environments.
    \item Developing a robust, lightweight and rugged hardware architecture to facilitate extreme exploration and high-level sensing, communications and computational necessities.
    \item \rev{Accomplishing live-communication between robots from deep within subsurface environments and back to a surface location, though this is not discussed at length in the paper.} %Tom: Just thought I'd throw this in here. Because we DO TECHNICALLY advance the state-of-the-art in this area in our work, we just don't want to talk about it in the paper. Can we just point to another one of our papers? 
    %\item Demonstrating real-world complexAddressing challenges in operating multi-robot simultaneously, exploring complex environments, and intermittent communication by deploying Nebula's mission planning modules
    %\item Discussing what we learned during design, testing, and operation of Nebula's powered Spot
\end{enumerate}
The performance of these technologies was successfully field-proven at the Urban Circuit of the DARPA SubT Challenge, propelling our team CoSTAR to a 1st-place victory.

\ph{Outline}
In Section \ref{sec:nebula}, we describe the elements of the NeBula architecture at its conceptual layer. 
In Section \ref{sec:spot}, we discuss the legged mobility system and the hardware payload. Sections \ref{sec:state_estimation}, \ref{sec:local_planning}, and \ref{sec:mission_planning} focus on selected algorithmic aspects of legged robot odometry, local planning, and high-level mission planning.
Experimental results are presented in Section \ref{sec:experiments}, followed by future work discussion and conclusions in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NeBula Autonomy}\label{sec:nebula}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Motivated by autonomous exploration of extreme surfaces and subsurface terrains on the \rev{Moon, Mars} and other planetary bodies, \rev{NASA's Jet Propulsion Laboratory (NASA JPL)} is developing an autonomy architecture referred to as NeBula (Networked Belief-aware Perceptual Autonomy). 
The main focus of NeBula is to provide computationally tractable methods to predict and assess various outcomes and risks in uncertain settings. \rev{These methods subsequently} enable reliable, coordinated multi-robot exploration of unknown and hard-to-access terrains. %6.5
%To deal with unknown environments and uncertain settings, NeBula takes a probabilistic approach to model uncertainty.

\rev{To deal with uncertainty in unknown environments, NeBula employs a probabilistic approach.
%NeBula adopts a probabilistic approach to model the uncertainty. 
It fuses various sensing modalities %with consideration for
\rev{while computing} uncertainty-awareness. NeBula then creates a probabilistic representation of the robot's knowledge of the environment, assigns risk values to \rev{successively} available actions, \rev{and then proactively plans ahead in order to minimize overall mission risk.}} %6.5

%It and takes the uncertainty into account to probabilistically fuse various sensing modalities, creates a probabilistic representation of the robot's knowledge of the environment, computes risk, and "proactively" plans to minimize the mission risk. 

\ph{Architecture} %NeBula consists of multiple components, briefly described in the rest of the paper.
Figure \ref{fig:spot_sysarch} illustrates a high-level overview of the NeBula architecture and how its modules are interconnected. 
Spot's internal locomotion model and inbuilt factory sensors, and NeBula sensors encapsulate the overall dynamics models of the system.
%The Spot locomotion and sensing module, and NeBula sensors encapsulate the dynamics models of the system, sensors. %\st{models}
\rev{Figure \ref{fig:spot_sysarch} will be discussed further in Section \ref{sec:spot}.}
The odometry module \rev{is responsible for measuring and estimating the state and relative motion of the robot.}
%contains algorithms to measure and estimate the relative motion of the robot.
We will discuss some aspects of odometry on legged systems in Section \ref{sec:state_estimation}. 
The estimated odometry is then used within the front-end of our global SLAM \rev{module} (detailed in~\cite{Ebadi2020}). 
\rev{The shared world belief block compiles the robot's model of the environment.}
The planning block, includes the 1) mission planning module that switches between various behaviors \rev{such as} exploration, stair-climbing, communication-recovery, etc., 2) global planning \rev{as well as} 3) \rev{traversability analysis and motion planning.} 
\rev{The belief prediction module project the belief from the feedback of planning modules.}
We will discuss these modules and highlight their uncertainty- and perception-aware aspects in Sections \ref{sec:mission_planning} and \ref{sec:local_planning}. 
The communication block is responsible for enabling data exchange between the multiple robots and a base station \rev{(described in~\cite{Otsu2020})}. 

% Backup
% \ph{Architecture} NeBula consists of multiple components, briefly described in the rest of the paper.
% Fig. \ref{fig:spot_sysarch} illustrates a high-level overview of this architecture and how its modules are interconnected. 
% The locomotion/sensing module encapsulates the dynamics models of the system, sensors, %\st{models}, 
% and the API to send commands to the robot and read its sensory data. 
% We will discuss this block further in Section \ref{sec:spot}.
% The odometry block \rev{is responsible for measuring and estimating the state and relative motion of the robot.}
% %contains algorithms to measure and estimate the relative motion of the robot.
% We will discuss some aspects of this block in Section \ref{sec:state_estimation}. 
% The estimated odometry is then used within the front-end of our global SLAM \rev{module}(detailed in \cite{Ebadi2020}). The planner block, includes the 1) mission planning layer that switches between various behaviors \rev{such as}exploration, stair-climbing, communication-recovery, etc., 2) global motion planning \rev{as well as}3) \rev{traversability analysis and local motion planning.}We will discuss these modules and highlight their uncertainty- and perception-aware aspects in Sections \ref{sec:mission_planning} and \ref{sec:local_planning}. 
% The communication block is responsible for enabling data exchange between the multiple robots and a centralized server\rev{(described in \cite{Otsu2020})}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Au-Spot Mobility System}\label{sec:spot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ph{Locomotion System} 
Spot is a quadrupedal robot developed by Boston Dynamics to provide mobility on challenging terrain, which cannot be accessed by traditional ground robots.
%in extreme environments and conditions. 
Spot can negotiate challenging non-planar terrain on a variety of surface types, including stair climbing.

%Spot can traverse various terrain types, including grass, sand, and gravel at slopes of up to $\pm$30~deg. 
%It has separate gaits for dynamically stable movement, uneven terrain, and stair climbing. %Spot can carry external payloads in excess of 14~kg operates for 90~minutes  (mixed usage). 

%NeBula has the ability to autonomously command Spot's locomotion in several aspects.

% It can carry about 14 kg of payload and it lasts about 90 min. Boston Dynamics offers a wide range of low--level motion planning algorithms. The developer can use the Boston Dynamic API to access internal sensor and map data and also send velocity of waypoint commands for the internal planner.

\ph{Sensing system} 
Spot's factory perception package from Boston Dynamics comprises five custom RealSenses distributed around the robot. Considering Spot’s overall traversability capabilities, mechanical constraints, and integration complexities, NASA JPL's NeBula Sensor Package (NSP) provides %a vehicle-agnostic and 
an elevated perception system (Fig. \ref{fig:spot_full_nebula_form}) to enable higher levels of autonomy. 
%is designed with consideration for Spot’s overall traversability capabilities, mechanical constraints, and the sensor requirements of NeBula autonomy (Fig. \ref{fig:spot_full_nebula_form}). 
The Nebula Sensor Package includes a LIDAR, Intel RealSense cameras, high-intensity LEDs, an IMU, gas and wifi detectors and a thermal camera.
These sensors are integrated into a shock-absorbing, rigid mechanical super structure. 
Spot is a fast-moving quadruped with the ability to traverse rough terrain and With NeBula autonomously driving this capability to its limit, the NSP can experience significant forces, moments, and vibrations. 
A combination of hard resin urethane, semi rigid carbon-infused nylon, and aluminum are used in the manufacturing process for increased structural integrity, sensor data fidelity and lightweight build. 
%As Spot is being used with NeBula as a research platform while pushing the envelope of the current state of the art of autonomous systems, 
Further, the design takes into consideration atypical load paths for shock absorption during falls. %Furthermore, the  sensor package needs to be adaptable to changing requirements. This is pertinent when competing in a DARPA challenge where the understanding of the competition and environment is rapidly evolving.
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.2\textwidth]{graphics/spot_sensor_package_NSP_2.jpg}
%  \caption{Spot with Nebula Sensor Package Mounted}
%  \label{fig:spot_payload}
%\end{figure}
Throughout this paper, we refer to the NeBula-powered Spot as 'Autonomous Spot' or 'Au-Spot' [/awe-spot/]. 
%(\textbf{Au}tonomous \textbf{S}ensing, \textbf{P}erception, \textbf{O}dometry and \textbf{T}raversability)


\ph{Power and Computing}
The NeBula Power and Computing Core (NPCC) is designed to mount onto Spot as an auxiliary payload which provides power to all of the NeBula sensors and the computers used for autonomy. The payload enclosure is designed with aluminum to provide protection to the internal electronics if Au-Spot were to fall. 
The payload is powered from an external lithium high capacity battery to provide isolation and extended battery life for Spot's internal battery. 
The NPCC also features a custom power distribution and safety module, which provides fuses, overcurrent protection, overvoltage protection, inrush current limiting and power sequencing of five high efficiency voltage regulators for the sensors, lights and computers. %In addition, the power module manages the physical and remote e-stops and safety systems for the vehicle. Communication to the human supervisor is achieved using a Silvus Technologies Streamcaster radio. High bandwidth communication between the computers, radio, LIDAR and Spot is done through Ethernet via an internal network switch. The cameras, IMU and UWB modules interface to the computers via USB through multiple powered hubs. 
The payload uses two high-power computers 
%(Intel and Nvidia) 
for sensing, autonomy and semantic scene understanding.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{graphics/spot_full_annotated_1.PNG}
  \caption{"Autonomous Spot:" Spot powered by NeBula (Au-Spot).}
  %in its full, NeBula powered form.}
  \label{fig:spot_full_nebula_form}
\end{figure}

%\begin{figure}[thpb]
 %  \centering
  %  \begin{subfigure}[t]{0.5\linewidth}
   %     \centering
    %    \includegraphics[width=\textwidth]{graphics/spot_sensor_package_NSP_2.jpg}
     %   \caption{Spot with Nebula Sensor Package Mounted}
      %  \label{fig:spot_payload_sens}
    %\end{subfigure}%
 %~
  %  \begin{subfigure}[t]{0.5\linewidth}
   %     \centering
    %    \includegraphics[width=\textwidth]{graphics/Spot_NPCC.jpg}
    %    \caption{Nebula Power and Computing Core with Covers Removed}
     %   \label{spot_payload_elec}
    %\end{subfigure}
    
%~
 %   \caption{NeBula Payload}
%\end{figure}

%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.3\textwidth]{graphics/Spot_NPCC.jpg}
%  \caption{Nebula Power and Computing Core with Covers Removed}
%  \label{fig:spot_payload}
%\end{figure}

% \inst{we dont need API discussion}
% \subsection{Spot Software Development API}
% \ph{Description of how program Spot through its accessible GRPC-based API and Python client library.}
% The Spot Software Development Kit enables your application to command poses and velocities, configure payloads, and access robot perception and payload data. Our Autonomy Software Development Kit, currently in Beta, will provide access to mapping, navigation, and mission editing. The API employs state-of-the-art security tools to keep your data secure.

% \ph{Robot State provided by the API and what we user}

% \ph{How to control the robot and what features are available}
% Locomotion parameter, allow obstacle avoidance, swing heights etc

% \ph{Conclude the capability of Spot and why we need NeBula to make Spot fully autonomous}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Software Interface to NeBula} % Nar5
% \inst{not very clear to me what this subsection is yet. Let's finish other parts and see if we need this}
% \ph{Overview of system integration}

% \ph{For system integration, we built the Spot driver to connect Spot API to ROS}

%\ph{Spot State}
% Robot state that we think its useful. KO, VO, Joint states, battery health

%\ph{Spot Mobility}
% Mobility setup that we think its useful: stair mode, selfright, friction coeff etc
% Stair climbing


% Spot system description will include:
% \begin{itemize}
%     \item System and locomotion system overview
%     \item Sensors description
%     \item Interfacing with Spot with API
%     \item Accessible States of the robot
%     \item Accessible locomotion, navigation feature adjustments and what we really care
%     \item Conclusion of the capability of Spot and why we need NeBula to have fully autonomous operation
% \end{itemize}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NeBula Odometry on Legged Systems}\label{sec:state_estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To enable autonomous robotic operation in extreme environments, a reliable odometry source is \rev{a necessity.} %a prerequisite. %In such scenarios, 
Darkness, presence of obscurants (e.g. dust, fog, smoke), self-similar areas, along with strong platform vibration caused by mobility-stressing terrains, are common features \rev{of extreme environments} which pose severe challenges to robotic perception, making odometry estimation a \rev{difficult problem} % cr
ucial challenge 
under perceptually-degraded conditions.

%\ph {Odometry Challenges} 
Uneven and slippery areas make inertial sensing inaccurate while the material composition of the surface where the legged robot is walking on (e.g soft moquette, hard concrete) has strong impacts on the accuracy of kinematic-based odometry (KO). Darkness, or sudden excessive change in illumination, along with dust and the occasional presence of fog and gas, pose significant challenges to cameras, and potential visual aliasing phenomena in texture-less or texture-repetitive environments make feature-tracking problematic, decreasing the overall reliability of vision-based odometry (VO). Self-similar environments with repetitive geometry and lack of distinctive landmarks, make scan-matching based methods ambiguous and prone to drift: moreover, active stereo cameras (including the in-built factory ones on the Spot platform) have a limited field of view, which renders them insufficient for our long-range perception applications.


\ph{Solution Architecture} To overcome these challenges, NeBula relies on a LIDAR-centric uncertainty-aware multi-sensor fusion framework where a \emph{selected} odometry source is fused as a prior with LIDAR information to enable accurate ego-motion estimation under challenging perceptual conditions. The main components of the proposed approach are: \texit{(i)} an anomaly-aware odometry multiplexer (HeRO), \texit{(ii)} a multi-sensor LIDAR-centric SLAM front-end (LOCUS) and \texit{(iii)} a SLAM back-end (LAMP). Fig.\ref{architecture} provides a high-level overview of the proposed approach. We discuss each component in the following.

\begin{figure}[thpb]
  \centering
  \includegraphics[width=0.48\textwidth]{spot_iros/graphics/architecture.pdf}
  \caption{\rev{Solution Architecture of the NeBula Multi-Sensor Fusion Framework}}
  \label{architecture}
\end{figure}



\ph{Odometry Multiplexer} To select the best odometry prior to be fused with LIDAR information, we feed multiple and heterogeneous sources of odometry available onboard (e.g., KO, VO, etc...) into an anomaly-aware odometry multiplexer, referred to as HeRO~\cite{Santamaria-navarro2019}. At every time step, HeRO runs a confidence test on each odometry \rev{stream \st{prior}} to detect potential anomalies %in the estimation stream 
(e.g., gaps, jumps, divergences) and identify~\cite{Santamaria-navarro2019} the most reliable input $\textbf{Y} \in SE(3)$ to be used as a prior in the \rev{\rev{LIDAR} \st{lidar-based}} front-end.
% input from a predefined ranking following a resilient paradigm: if an anomaly is detected in the highest-ranking source, then the next source in the ranking that passed the confidence check is selected. %Being $\mathcal{R}$ the coordinate system of the robot and $\mathcal{W}$ the coordinate system of the world, 
% In the following we indicate the pose estimate of HeRO's output with $\textbf{Y} \in SE(3)$.



\ph{Localization Front-End} The output of the odometry multiplexer is fed into a multi-sensor LIDAR-centric SLAM front-end module, referred to as LOCUS~\cite{Palieri2020} that performs a cascaded GICP-based scan-to-scan and scan-to-submap matching operation to estimate the relative motion of the robot between consecutive LIDAR acquisitions. Let $L_{k}$ denote the \rev{LIDAR} scan acquired the $k$-th time step. We indicate with $\textbf{E}^{k-1}_{k} = \textbf{Y}^{-1}_{k-1}\textbf{Y}_k\in SE(3)$ the rigid body transformation of HeRO's output between two consecutive LIDAR acquisitions. %(in order to address asynchronous odometry measurements, we interpolate the buffered $\textbf{Y}$ at LiDAR timestamps to $t_{k-1}$, and $t_k$ to get $\textbf{Y}_{k-1}$ and $\textbf{Y}_{k}$: the pose of HeRO's output at times $t_{k-1}$ and $t_k$, respectively). 

In the scan-to-scan matching stage, GICP computes the optimal transformation $\hat{\textbf{T}}^{k-1}_{k}$ that minimizes the residual error $\mathcal{E}$ between corresponding points in $L_{k-1}$ and $L_{k}$. 
\begin{align}\label{eq:GICP_ss}
\hat{\textbf{T}}^{k-1}_{k} = \arg\min_{\textbf{T}^{k-1}_{k}} \mathcal{E} (\textbf{T}^{k-1}_{k}L_{k}, L_{k-1})
\end{align}
To enhance accuracy we initialize the optimization with $\textbf{E}^{k-1}_{k}$. In the case where no input is received by HeRO, we rely on the identity transformation as the prior and the system reverts to pure \rev{LIDAR} odometry. 


To enable global consistency across the history of scans, the motion estimated in the scan-to-scan matching stage is further refined by a scan-to-submap matching step. 
%Here $L_k$ is matched \rev{to} the global \rev{map} $M_k$ given the current estimate of the robot pose in the world frame.
\rev{Here $L_k$ is matched to a local submap $S_k$ which is taken from the local region of the global map given the current estimate of the robot pose in the world frame.}
% (the global map is an accumulation of point clouds after every $t$~meters of translation, or $r$~degrees of rotation).



%\rev{To enable global consistency across the history of scans, the motion estimated in the scan-to-scan matching stage is further refined by a scan-to-submap matching step. Here $L_k$ is matched against a local submap $S_k$ which is taken from the local region of the global map $M_k$ given the current estimate of the robot pose in the world frame (the global map is an accumulation of point clouds after every $t$~meters of translation, or $r$~degrees of rotation). %and is stored in an octree format: in our results, we use $t = 1$, $r = 30^o$).
\begin{align}\label{eq:GICP_map}
\tilde{\textbf{T}}^{k-1}_{k} = \arg\min_{\textbf{T}^{k-1}_{k}} \mathcal{E} (\textbf{T}^{k-1}_{k}L_{k}, S_{k})
\end{align}
The initial point of this optimization is $\hat{\textbf{T}}^{k-1}_{k}$ resulted from Eq. (\ref{eq:GICP_ss}).
% This optimization is initialized with the $\hat{\textbf{T}}^{k-1}_{k}$ result from (\ref{eq:GICP_ss}). 
After scan-to-scan and scan-to-submap matching, the final estimated motion $\tilde{\textbf{T}}^{k-1}_{k}$ between consecutive LIDAR acquisitions is used to update the robot pose in the world.%: the generated odometry is therefore the integration of all computed incremental transforms. 
% Both accuracy and computational speed are improved by the cascaded optimization approach: a good initial estimate in both (3) and (4) reduces the chances of converging in a sub-optimal local minima, and reduces the number of iterations needed to converge, lowering computation time.

%The key idea is to use the incremental transformations $\Delta T_{k} = T_kT^{-1}_{k-1}$ coming from the external odometry source to seed the Generalized Iterative Closest Point (GICP) algorithm in the scan-to-scan registration stage. More specifically, we initialize GICP between the Lidar scan $L_k$ collected at the current time $k$ and the LiDAR scan $L_{k-1}$ collected at $(k-1)$-th time step with the rigid body transformation resulting from the external odometry between time steps $k-1$ and $k$, namely $\Delta T_{k}$ . We then use this accurate external odometry incremental transform as the initial guess for GICP, which computes the optimal transformation $T'_{k} $ that minimizes the distance $d$ between the scan $L_{k-1}$ and the scan $L_k$ transformed by $\Delta T_{k}$. %To further enhance the accuracy of the scan-to-scan estimate, we also use a scan-to-submap matching refinement step, where the latest scan $L_k$ is matched against a local submap $M_k$.

\ph{Localization Back-End} The odometry produced by the front-end is fed into the back-end of our SLAM system, referred to as LAMP~\cite{Ebadi2020} which receives pose-to-pose constraints and solves a Pose Graph Optimization (PGO) and Incremental Consistency Measurement (ICM) problem for global localization when loop closures are detected during traverse.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Local Planning}\label{sec:local_planning}
This section describes our approach to enable Spot to traverse challenging environments, where assessing the traversability risk and planning safe paths is a prerequisite for autonomous navigation.
%\rev{In order to accomplish the overall mission, we develop the local planner is to choose actions that minimize the failure risk to the robot.}

%\rev{In order to \rev{accomplish}the overall mission, the robot must negotiate its local surroundings and safely navigate to local waypoints. %6
%While time or distance minimization are common objectives for a local planner, we put a significantly higher emphasis on the robot's safety.
%As we are operating in unknown and challenging environments, the main objective of the local planner is to choose actions that minimize the failure risk to the robot.}

%To this end, we adopt a risk-aware local planning approach, where we analyze the traversal risk of the local surrounding and plan minimum-risk paths to a given waypoint.

%In our \rev{mission of interest,}traversability challenges include 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Traversability Map}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We discretize the belief of the map $M$ into $N_M$ grid cells, by constructing a discretized random field in multi-dimensional space, where each independent cell stores a probabilistic estimate of the map.  For example, the cell can capture belief about occupancy, risk of traversal, or the likelihood it contains an artifact or object of interest.  Occupancy grid maps have been well studied for some time \cite{moravec1985high,elfes1990stochastic}.
% %
% Formally, an we define the grid map as composed of grid cells:
% \begin{align}
%   M = (\mathbf{m}^1, \mathbf{m}^2, \dots, \mathbf{m}^{N_M}),
% \end{align}
% where $\mathbf{m}^i$ is a random variable for the state of the $i$-th cell,
% i.e., $\mathbf{m}^i \in \mathbb{M}$, and
% $N_M$ is the total number of grid cells in the map.
% In the case of an occupancy grid map, $\mathbf{m}^i$ is a Bernoulli random variable with $\mathbb{M} = \{free, occupied\}$.  More generally, $\mathbb{M}$ can be the cross space of multiple boolean and real variables which represent different attributes.  For example, $\mathbb{M} = \{free, occupied\}\times\{artifact, no\, artifact\}\times [0,1] (risk)$.
% The value of $\mathbf{m}^i$ is often assumed to be static over time.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Consider a traditional mapping formulation for coverage. The belief state in this case is $b = p(M, x)$ where $M$ is the map state and $x$ is the robot state. We model $M$ as an occupancy map composed of grid cells  $m = \{\textbf{m}_i\}$ where $\mathbf{m}_i$ is a random variable representing some state (i.e. occupancy, coverage) of the $i$-th cell. In order to compute the posterior probability $p(M | z_{1:t}, x_{1:t})$ of the whole map $M$, given the robot's measurements $z_{1:t}$ and trajectory $x_{1:t}$, the cell states are traditionally approximated as binary with full independence between them \cite{TBF05,elfes1990stochastic}. %6
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We model the traversability map $g=(m^1,\cdots,m^n)$ as a grid of cells $m = \{\textbf{m}^i\}$ where $\textbf{m^i}$ is a random variable representing the traversability state of the $i$-th cell. 
% X = {fail, safe}, X ~ Ber(m)
% We discretize the risk belief of the map $M$ into $N_M$ grid cells, by constructing a discretized random field in multi-dimensional space, where each independent cell stores a probabilistic estimate of the map. For example, the cell can capture belief about occupancy, risk of traversal, or the likelihood it contains an artifact or object of interest. 
% In order to balance efficient traversal and low risk to the platform, we construct a measure of accumulative risk for a given path through the environment.
We model the traversability of the environment as a grid map $g=(m^1,\cdots,m^n)$ with $n = n_l \times n_w$ cells, arranged in a \textit{locally} 2D grid, where $m^i\in \{safe,lethal\}$ is a Bernoulli random variable.  $m^i=safe$ represents the event that the robot can safely navigate through the $i$-th cell. 
We infer the probability distribution $p(g)$ over grid $g$ and store it as a traversability map.
The steps involved in computing $p(g)$ are detailed in this subsection.

% We model the traversability of the environment as a \rev{planar} map $g=(m^1,\cdots,m^n)$ over a grid of cells $G = \{i^1, i^2, \cdots, i^n\}$, where $i^j$ defines the location of the location of the $i$-th cell. $m^i$ is a Bernoulli random variable, with $m^i \in \{safe,lethal\}$ representing that the robot can safely navigate through the $i$-th cell. 
% We estimate the traversability posterior over the map $p(g)$ and store it as a traversability map.
% The steps involved in computing $p(g)$ are detailed in this subsection.

% Let $g=(m^1,\cdots,m^n)$ be a \rev{robot-centric}grid of $n=n_l\times n_w$ cells with length $n_l$ and width $n_w$, and let each $m^i\in[0,1]$ represent the probability that the robot safely traverses through cell $i$. 

%Let $\pi=\{i^t\}_{t=0}^T$ denote a path defined as a sequence of connected cells $i^t \in g$. \rev{The set of all paths that connect the current robot pose with the goal is denoted by $\Pi$.}
%\ph{Path Risk}We define the total accumulated risk of the path $\pi\in\Pi$ as 
%\begin{align}
% \mathcal{R}^{\pi}=1-\prod_{t=0}^Tm^{i_t}
%\end{align}
%which is the probability that the robot fails to safely traverse the path $\pi$.  

\ph{Risk Sources}
There is a variety of traversability-stressing elements which increase the probability of failure during traversal.
These elements can be purely geometric (e.g., positive obstacles, negative obstacles, steep slopes) or of semantic nature (mission items such as deployed communication nodes or other robots).
%The most impactful sources, which are therefore considered in our analysis, include a) positive obstacles, b) negative obstacles, c) slopes, d) steps, and e) \rev{mission items}(other robots, deployed communication nodes).
%, and f) sensor blind spots.

% We combine multiple layers of grids that encode different types of risk. Assuming mutual exclusivity between each risk type, the total risk in each grid cell is simply the product of the individual risks of each layer.  We consider the following sources of risk:
% \begin{itemize}
%     \item Obstacles (Walls, Rubble, etc.)
%     \item Steep Slopes
%     \item Negative Obstacles
%     \item Sensor Blind Spots
%     \item Mission Items (comm nodes, other robots, etc.)
% \end{itemize}

\ph{Multi-Fidelity Terrain Map}
% To identify these sources of risk, we perform traversability analysis on a local, robot-centric terrain map.
% In order to trade-off \emph{range}, \emph{coverage} and \emph{accuracy}, we construct a multi-range/multi-fidelity terrain map.
% \ph{Short range}
% On the short range ($<$ 1m), we mainly rely on the height-map created by Spot's internal depth cameras.
% For the medium range (5mx5m), we use instantaneous LIDAR point clouds.
% For the long-range (20mx20m), we build a spatial map \cite{oleynikova2017voxblox} by integrating LIDAR point clouds with odometry.
% On the short range, a step filter is applied to mark points whose distance from the local ground plane is above or below the  robot's ground clearance.
% \ph{Mid-long-range}
% On the medium and long range, we perform ground segmentation \cite{himmelsbach2010fast} and settling-based collision and stability check  \cite{krusi2017driving}.
% Settling is performed by fitting local ground planes in the point cloud terrain map. Additionally, gaps in the medium-range map are marked as negative obstacles.
% $g = \{g^{1},...g^{n}\}$
% Each risk source a.) - f.) is stored in its own risk submap $\{g_{a},...g_f\}$. 
% The maps are resolved at 2cm/cell for the medium range and 5cm/cell for the long-range.
% Deployed nodes are hazards with known locations. Blind spots correspond to chunks of unknown space in the short-range terrain map.
For detecting the aforementioned traversability-stressing elements in unknown environments, we build a local terrain map of the robot's surrounding, via data collected from NSP. % for modeling the terrain up to 10m away from the robot.
Specifically, we build a multi-fidelity map at various ranges. We use depth cameras for short-range sensing, %($\leq$1m), 
instantaneous LIDAR point clouds for medium-range sensing, %($\leq$3m), 
and spatially fused point clouds \cite{oleynikova2017voxblox} for long-range detection. % ($\leq$10m). 
The combination of these various sensing capacities yields an efficient trade-off among \emph{range}, \emph{density} and \emph{accuracy} in the resulting terrain map.

\ph{Multi-Layer Traversability Map (MLT)}
To capture various traversability-stressing elements, we construct a layered representation (Fig. \ref{fig:layered_costmap}) of the traversability map $g=\{{}^{1}g,...,{}^{N}g\}$, where each layer ${}^{\ell}g$ captures a certain risk source (as explained above). %The vertical dimension separates the traversability map by risk source and the horizontal dimension separates it by the fidelity level.
%Each identified by performing traversability analysis on different parts of the multi-fidelity terrain map. 
At each layer ${\ell}$, we apply algorithms and perform an analysis relevant to the corresponding risk element. 
Specifically, positive and negative obstacles as well as steep slopes are detected on the dense short-range by applying a step filter relative to the local ground plane.
On the medium- and long-range: %, steep slopes, positive and negative obstacles are detected by different methods: 
\textit{(i)} Positive obstacles are detected by performing ground segmentation \cite{himmelsbach2010fast} and settling-based collision checks \cite{krusi2017driving}, \textit{(ii)} Negative obstacles by searching for surface discontinuities (holes) in the LIDAR point cloud, and \textit{(iii)} steep slopes by using settling methods such as \cite{krusi2017driving}. Mission items (e.g., deployed communication nodes and other robots) can be detected semantically and superposed on the traversability map.
%To detect positive and negative obstacles, a step filter was applied to the dense, short-range terrain map created by visual data. In the medium range map, we detect and classify disconnected components in the lidar range scan to identified surface discontinuity and negative obstacles.
%A step filter was applied to the dense short range terrain map to detect positive and negative obstacles.
%On the sparser medium- and long-range map, ground segmentation \cite{himmelsbach2010fast} and settling-based collision and stability checks \cite{krusi2017driving} were applied to detect positive obstacles and high slope regions. Mission items (e.g., deployed communication nodes and other robots) can be detected semantically and superposed on the traversability map.


%This architecture is henceforth referred to as the \rev{multi-layer/multi-fidelity traversability map (MLMFTM).}


%hence the range.
%inspired by the  
%We will refer to this architecture as the \emph{risk-range volume}, inspired by the 
%\emph{scale-space} architecture known within the computer vision community.
%Different sources of risk have to be detected by different methods. In addition to that, the heterogeneity of the terrain map requires us to use different methods for the \emph{same} risk type.
% To detect positive and negative obstacles, a step filter was applied to the dense, short-range terrain map created by visual data. In the medium range map, we detect and classify disconnected components in the lidar range scan to identified surface discontinuity and negative obstacles.
% %A step filter was applied to the dense short range terrain map to detect positive and negative obstacles.
% On the sparser medium- and long-range map, ground segmentation \cite{himmelsbach2010fast} and settling-based collision and stability checks \cite{krusi2017driving} were applied to detect positive obstacles and high slope regions. Mission items (e.g., deployed communication nodes and other robots) can be detected semantically and superposed on the traversability map.

%The remaining two risk sources, deployed communication nodes and other robots, did not need a special analysis as their locations are already known.

%How the individual layers feed into the overall traversability map $g$ is elaborated further below.
% \begin{figure}[thpb]
%   \centering
%   \includegraphics[width=0.2\textwidth]{spot_iros/graphics/layered_costmap.png}
%   \caption{The mulit-layer/multi-fidelity traversability map MLT}
%   \label{fig:layered_costmap}
% \end{figure}
\begin{figure}[thpb]
  \centering
  \includegraphics[width=\linewidth]{spot_iros/graphics/MLT.png}
  \caption{\ali{needs a longer caption} \rev{The multi-layer traversability map (MLT):}}
  \label{fig:layered_costmap}
\end{figure}

% \begin{figure}[thpb]
%   \centering
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[0.24\textwidth]{spot_iros/graphics/MLT.png}
%     \end{subfigure}%
%     ~
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[trim={1cm 1cm 1cm 1cm},clip, width=\linewidth]{spot_iros/graphics/spot_mission.png}
%     \end{subfigure}
%     \caption{Left:  Risk map generated from Spot's onboard cameras with lethal (pink), high-risk but non lethal (red), and low risk (blue) regions. Right: Au-Spot risk map layer combined with LiDAR based risk map showing larger range of obstacles for a more robust and long-range planning.}
%     \label{bd_costmap}
% \end{figure}


% \begin{figure}[thpb]
%   \centering
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[trim={4cm 3cm 4cm 3.5cm},clip, width=\linewidth]{graphics/costmap_bd2.png}
%     \end{subfigure}%
%     ~
%     \begin{subfigure}{0.5\linewidth}
%         \centering
%         \includegraphics[trim={1cm 1cm 1cm 1cm},clip, width=\linewidth]{graphics/costmap_bd2_embedded.png}
%     \end{subfigure}
%     \caption{Left:  Risk map generated from Spot's onboard cameras with lethal (pink), high-risk but non lethal (red), and low risk (blue) regions. Right: Au-Spot risk map layer combined with LiDAR based risk map showing larger range of obstacles for a more robust and long-range planning.}
%     \label{bd_costmap}
% \end{figure}

\ph{Fast Traversability Evaluation}
To enable online generation of MLT as the robot moves, we perform traversability evaluation only on a representative and sparse set of sampled cells $Q = \{i_s\}_{s=0}^S$ in the grid map. %$Q$ can for example be selected by random sampling or by applying a lattice structure to the MLMFTM.
% The result is a sparse set of cells $Q$ within the MLMFTM, for which the exact traversability risks $\{p({}_{\ell}m^{i_t}=lethal)\}_{t=0}^T$ are computed for each layer $\ell \in N$.
On each sampled cell $i\in Q$, the traversability risks $p({}^{\ell}m^{i}=lethal)$ are computed and stored in all layers $\ell \in N$. To compute the traversability risk at every cell in the grid, % (including the ones outside the sampled point regions...), }
%When planning smooth paths, high resolution
%it is desirable to have a high resolution 
%in the \rev{spatial} dimension of the MLT,is desirable.
we approximate the MLT at a higher resolution by interpolating with a Gaussian kernel or by adding inflation with a radial decay to each query cell $i \in Q$ along the spatial dimension.
Thus, we compute the traversability $p({}^{\ell}m^{n}=safe)$ for all cells $n$ in all layers $\ell$.
% \begin{align}
%     p({}^{\ell}m^{i}=safe) \hspace{10pt} \forall {\ell}=1,\cdots,N, \hspace{5pt} \forall i = 1,\cdots,n 
% \end{align}

% Because of computational constraints, traversability can not be assessed for every grid cell of the map.
% Instead, traversability is only checked for a sparse set of query points.
%Hard thresholds are used for the sources a.) - c.) in order to assign binary risk values $r \in \{0,1\}$. 

%A decaying inflation layer is added to any lethal point ($r=1$), which results in smooth risk maps.
% Sensor blind spots contribute a non-lethal risk and are identified by the respective sensor models.  
%Deployed communication nodes, other robots and sensor blind spots are added as blobs of high risk to their respective risk maps $g_e$ and $g_f$.

%\ph{Combined Risk}
\ph{Superposition}
For action generation, we create a single traversability map $g$ by fusing the $N$ different layers $\{{}^{1}g,...,{}^{N}g\}$. % for the aforementioned risk sources.
% With consideration for planning purposes,
% %For planning purposes, 
% a single traversability map $g$ is created from the different layers ${}^{\ell}g$.
The information in these layers are not independent in general. We approximate a conservative risk estimate by element-wise multiplication of traversability probabilities:
\begin{align}
p(m^i=safe) = \prod_{\ell=1}^Np({}^{\ell}m^{i}=safe)\hspace{5pt} \forall i = 1,\cdots,n
\end{align}

%The multiplication of the individual risk values does not imply probabilistic independence, but rather gives us a conservative estimate for the combined risk.

Bottom layer in Fig. \ref{fig:layered_costmap} illustrates an example traversability map (plotting the maximum likelihood estimation of $p(g)$), obtained during a real mission.

% \begin{figure}[thpb]
%   \centering
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[trim={1cm 1cm 0cm 1cm},clip, width=\linewidth]{graphics/costmap_image.png}
%     \end{subfigure}%
%     ~ 
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[trim={1cm 1cm 1cm 2cm},clip,width=\linewidth]{graphics/costmap_large.png}
%     \end{subfigure}
%     \caption{\rev{Left: Left camera image of a high-risk traversable region.  Right:  Risk map with obstacles in image to the left of the robot (centered); 1m markings are in gray.}}
%     \label{global_costmap}
% \end{figure}


%Accurate risk assessment is contingent upon good localization/odometry.  When odometry fails we want our assessment of risk to remain conservative in such a way as to keep the vehicle safe at all times.  Therefore we rely on a multi-resolution approach for assessing risk.  We asses risk on a local scale (2m radius) by relying on wide-field of view sensor data with a short temporal window.  This approach relies on a short history of KO/VO odometry, which tends to have low drift at these timescales.  For areas outside this small radius (2m-10m), we rely on temporally fused sensor data using our LiDAR payload, since this sensor has much longer range and higher accuracy at these scales.  We combine these two layers to achieve both long-range mapping for efficient planning, along with high-fidelity mapping at short ranges to ensure the safety of the platform.
%\ph{Solver}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertainty and Perception-aware Planning} 
% \ph{Uncertainty-Aware Planning}
To enable traversability through narrow passages and obstacle-laden environments, one needs to reduce the map uncertainty. We address this challenge by taking into account uncertainty which comes from noisy, asymmetric, and limited sensors in order to command the robot along trajectories that \rev{not only find the shortest path to the goal, but also reduce uncertainty in the map $p(g)$ so as to increase the probability the path taken will be safe.}. In the NeBula \rev{architecture}, we formalize this notion with an uncertainty-aware planner.  This general framework can be used to create behaviors which intelligently reduce risk which comes from uncertainties in sensing and the environment.  We outline the general framework here.

%We search for the minimum risk path using A*.
%The grid representation of the traversabiltiy map allows us to search for such a path using \rev{graph-search}algorithms,  such as A*.
%\rev{A waypoint is selected along the path and is sent to Au-Spot's proprietary planner for execution.}
%Having found the minimum risk path on the risk map, we then send a carrot waypoint 1m along this path to Spot's internal trajectory planner. 
%The proprietary planner keeps it's own small 5mx5m costmap, which is created from the platform's built-in depth cameras.
%Spot then executes the planned trajectory through its low-level controllers.
%\ph{Stair Climbing}
%A special case that is not handled by the local planner is stair climbing. \rev{Stairs will either appear as positive or negative obstacles in the risk map.} In the current setting, stair climbing is semi-autonomous. The human supervisor has to \rev {: \textit(i)} detect the presence of a stair case \rev{(e.g. from the robot Camera/LiDAR feed)}, \rev {\textit(ii)} switch Spot's internal planner to 'Stair' mode which configures the planner for climbing stairs, and then \rev {\textit(iii)} give \rev{a goal} at the end of the stair case. The actual climbing is then completely handled by Spot's internal planner and controller.
%\subsection{Perception-Aware Planning}

% To compute the risk, we take the robot's perceptual capabilities into account. 
% In particular, this is important for the Spot robot due to its asymmetric and limited sensory field of view. %., it is preferable to command the robot to move in the forward direction, where sensor coverage is optimal.  

\ph{Uncertainty-aware Representation}
Let $\mu^i$ be the mean of the Bernoulli distribution of $m^i$.  Then we can model the distribution (or our belief) of $\mu^i$ with its parameters, $p(\mu^i|\hat{\mu}^i,\sigma^{i})$ (e.g. a mean $\hat{\mu}^i$ and variance $\sigma^i$ in the case of a beta distribution). The ``confidence" about $\mu^i$ is captured in $\sigma^i$, where fully unknown and fully known cells have the highest and lowest $\sigma^i$ values, respectively \cite{CRM}. 

\ph{Map prediction}
The uncertainty-aware representation allows us to incorporate perceptual capabilities %(hence the name "perceptual" in NeBula) 
into the planning. We define a $K$-step policy \rev{$\pi_{0:K}$ that returns} an ordered sequence of grid locations that the robot visits, along with the orientations of the robot at those locations: \rev{$\pi_{0:K}(\cdot)=\{i_k, \theta_k\}_{k=0}^K$}. Given the sensors available on the robot and their configuration and noise characteristics, along with a given policy $\pi$, we update our belief of the traversability values in a recursive manner, which we call $\tau$:
% \rev{we can derive a recursive filtering model $\tau$ that predicts the evolution of $(\hat{\mu}^i,\sigma^i)$}based on the sensor configuration and most likely measurements along a given trajectory $\pi$ with \rev{$k=0,\cdots,K$ time steps:} 
% \begin{align}
%  \sigma^i_k = \tau( \sigma^i_0, z_{0:k}(\pi) )
% \end{align}
\begin{align}
 (\hat{\mu}^i_k, \sigma^i_k) =\tau(\hat{\mu}^i_{k-1}, \sigma^i_{k-1}, z_{k}(\pi) )
\end{align}
where the measurement $z_k(\pi)$ is predicted from a generative model, at the $k$-th time step along the trajectory $\pi$. This becomes increasingly important when the sensor configuration is highly asymmetric on a robot, which is the case for Spot as it has blind spots and areas where sensory measurement noise is considerably higher than other areas.
Maintaining separate probability distributions for individual cells in the map, we predict the map $p(g)$ for $k$-th time step into the future as follows: 
% \begin{align}
%  g_k = \{p(m|m^1_k,\sigma^1_k),\cdots,p(m|m^n_k,\sigma^n_k)\}   
% \end{align}
% \begin{align}
%  g_k = \{p(\mu^1|\hat{\mu}^1_k,\sigma^1_k),\cdots,p(\mu^n|\hat{\mu}^n_k,\sigma^n_k)\}   
% \end{align}
\begin{align}
 g_k \equiv \{(\hat{\mu}^1_k,\sigma^1_k),\cdots,(\hat{\mu}^n_k,\sigma^n_k)\}   
\end{align}
% where $p(\mu^i|\hat{\mu}^i_k,\sigma^i_k)$ is the probability distribution of $\mu^i$ at the $k^{th}$ time step parametrized by $\hat{\mu}^i_k$ and $\sigma^i_k$.

\ph{Risk-Aware/Perception-Aware Planning}
Next we define a risk measure that takes perceptual capabilities and uncertainties into account when planning trajectories.  We also define an optimal policy $\pi^*$ as the policy which minimizes the total path risk $\mathcal{R}^\pi$ along the $K$-step path while \rev{moving towards a given goal}:
% \begin{align}
%  \mathcal{R}_k^{\pi}=1-\prod_{t=0}^Tp(m|m_k^{i_t},\sigma_k^{i_t}),~~~
%     \pi^* = \arg\min_{\pi\in\Pi}\mathbb{E}[\mathcal{R}_k^{\pi}(g_k)]
% \end{align}
\begin{align}
 \mathcal{R}^{\pi} &= 1 - \prod_{k=0}^K p(m^{i_k}=safe | \hat{\mu}_k^{i},\sigma_k^{i})
 \label{eq:risk}\\
%  &= 1 - \prod_{k=0}^K\int_0^1 p(m^{i_k} = safe|\mu)p(\mu|\hat{\mu}_k^{i},\sigma_k^{i}) d\mu\\
 \pi^* &= \arg\min_{\pi\in\Pi}\mathbb{E}[\mathcal{R}^{\pi}]
 \label{eq:policy}
\end{align}

Efficient methods for computing predicted risk uncertainty over a 2-D grid for a given sensor model have been considered in~\cite{heiden2017planning}.
%Ali-akbar Agha-mohammadi, Eric Heiden, Karol Hausman and Gaurav S. Sukhatme, “Confidence-rich 3D Grid Mapping: Toward High-speed Vision-based UAV Navigation,” International Journal of Robotics Research (IJRR), vol.38, pp.1352-1374, 2019.
\rev{When computational complexity is a challenge (with respect to the robot speed), to  enable  efficient  real-time  computation  of  Eq.  (7),  we  rely  on  a  cascaded  policy,  where one can optimize for position and orientation (along the path) sequentially.}

% \ph{Real-Time Planning}
% %1 Let $\pi=\{i_k\}_{k=0}^K$ denote a path defined as a sequence of connected cells $i_k \in g$. \rev{The set of all paths that connect the current robot pose with the goal is denoted by $\Pi$.}
% %Heurstics in practice:
% \rev{To enable efficient real-time computation of Eq. (\ref{eq:policy}), we rely on a cascaded policy, where we decompose the policy $\pi$ into position and orientation $\pi = (\pi_p,\pi_\theta)$.  

% We can reduce the search space $\Pi$ by computing the minimum-risk path $\pi_p^*$ based on the prior distribution $p(g|\mu=\hat{\mu})$:

% Let $\pi_p=\{i^t\}_{t=0}^T$ denote a position path defined as a sequence of connected cells $i^t \in g$. The set of all position paths that connect the current robot position with the goal position is denoted by $\Pi_p$.

% We define the total accumulated risk of the position path $\pi_p\in\Pi_p$ as 
%  \begin{align}
%   \mathcal{R}^{\pi_p}=1 - \prod_{t=0}^Tp(m^{i_t}=safe|\mu^{i_t}=\hat{\mu}^{i_t})
%   \label{eq:accumulated_risk}
% \end{align}

% Then, the minimum risk path $\pi_p^*$ is defined as:

% \begin{align}
%   \pi_p^* = \arg\min_{\pi_p\in\Pi_p}\mathcal{R}^{\pi_p}
%   \label{eq:risk_planning}
% \end{align}

% The grid structure of $g$ enables us to use a graph-search method, such as A*.

% The new, reduced, optimization problem can therefore be written as:
% \begin{align}
%  \pi_{grid}^* = (\pi_p^*,\pi_\theta^*) = \arg\min_{\pi\in\Pi|\pi_p = \pi_p^*}\mathbb{E}[\mathcal{R}_K^{\pi}]
%  \label{eq:decoupled}
% \end{align}

% For the legged Au-Spot platform considered in this paper, we choose the orientation of $\pi_\theta$ to be positively tangential to $\pi_p$, which restricts the policy search space $\Pi$.
% This is motivated by Au-Spot's asymmetric field-of view, where the sensor coverage in the front is superior to every other direction with the minimum blind spot size.
% Intuitively, this heuristic will cause Au-Spot to point forward in the direction it moves, which leads to a larger reduction in uncertainty.
% To further speed up computation when building our map representation, we can choose to ignore the effect of future observations along a path $z_k(\pi)$.  Then $g_k$ remains static for $k=0,\cdots,K$, and efficient search for $\pi_p$ can be performed with optimal graph-based search methods.
% }

% For the legged Au-Spot platform considered in this paper, a simple heuristic that that approximates Eq. \ref{eq:decoupled} consists of choosing the orientations in $\pi_\theta^*$ to be positively tangential to $\pi_p^*$.
% %turned out to be a simple heuristic .
% %The heuristic consists of choosing the sequence of orientations $\pi_\theta^*$ to be the positive tangent to the found minimum-risk position path $\pi_p^*$.
% This is motivated by Au-Spot's asymmetric field-of view, where the coverage in the front is superior to every other direction, especially the back where sensor blind spots exist.
% Intuitively, this heuristic will cause Au-Spot to walk forward, which will lead to maximum information gain.}

% We define the total accumulated risk of the path $\pi\in\Pi$ as 
%  \begin{align}
%   \mathcal{R}^{\pi}=1 - \prod_{t=0}^Tp(m^{i_k}=safe)
% \end{align}
% which is the probability that the robot fails to safely traverse the path $\pi$.  
% $\pi^* = \arg\min_{\pi\in\Pi}\mathcal{R}^{\pi}(g)$.

%In a second, step the remaining part of the policy (heading) is found which solves Eq. \ref{eq:risk}.}

%We first use a pre-defined policy that maps the direction of walking to the robot heading. Then 

  
%To realize this behavior, in the case of integration of NeBula with Spot, we found that a simple heuristic sufficed to approximate the optimal solution $\pi^*$.  
%This heuristic was to solve for deterministic A* paths (Equation \ref{deterministic_plan}) and then to give a desired location and orientation to Au-Spot as a waypoint along this path, such that the relative heading between the current robot position and the waypoint itself is minimized. 

\ph{Execution}
We execute the planning problem outlined in Eq. \ref{eq:policy} in a receding-horizon control (RHC) fashion: We optimize for a $K$-step policy. Then, when executing the policy, we select a waypoint at a distance $d$ along the path $\pi^*$, send it to the robot, and while robot is moving towards the waypoint, we resolve the path planning problem to generate a new path from the new robot position. Selecting $d$ is a trade-off between smoothness and tracking error, where a larger $d$ improves stability and smoothness, while a smaller $d$ keeps tracking error of $\pi^*$ lower. \rev{The combined effect of perception-aware planning and RHC-based execution will cause Au-Spot to prefer moving in directions that lead to richer sensory input (in our sensor configuration, the forward direction), which leads to a larger reduction in uncertainty via collecting more task-relevant information.}


% There are two contradicting requirements when choosing the parameter $d$:
% \rev{\begin{enumerate}
%     \item Spot runs a proprietary planner, independent of NeBula, which has much shorter range than what NeBula can deliver. If $d$ is chosen too big, we have no guarantee that the executed path will be close to $\pi^*$ anymore.
%     \item When exploring unknown environments, the planned path $pi^*$ can jump between planner iterations, due to newly observed obstacles. Making $d$ too small will lead to jumps in the chosen waypoint as well. This is especially problematic as Spot is an omnidirectional robot, which can sidestep.
% \end{enumerate}}

%We recursively  continue this process. This results in a behavior where the robot orients itself to a direction that maximizes the sensory input relevant to the task in hand (in this case minimizing the failure probability) whenever possible. The RHC way points along the path are chosen by trading off several parameters including: 1) avoiding local minima and increasing perception-awareness which requires more frequent and nearby waypoints, and 2) to ensure the SPOT stability and smooth motion the waypoints needs to be sufficiently spaced out.

%Holonomicity of the robot allows us it to orient itself in any direction as it moves.  
%Furthermore, the sensor coverage from Spot's internal cameras is good in the near-field, while the NeBula Sensor Package LiDAR is good in the far field. 

%\ph{Specific example Necessity of perception aware planing in Spot}
%Although a key feature of NeBula is in planning perception-aware paths, 


%We combine these two streams of data to compute risk (see Fig. \ref{bd_costmap}).  
%Future work will involve bringing the more advanced uncertainty-aware planning from the NeBula framework into Spot. 



% \begin{itemize} % For Mission Autonomy(Behavstmap) -> iours, IRM) probably we just gonna cite Scott and Kyon aero paper. 
%         \item Traversability (settling, collision check, slope check, negative obstacles,..) (David?)
%     \item Local Planning (our traversability/local planner complements BD's internal one. We have long range LiDAR, bigger costmaps and detect negative obstacles. In the local planner, we can encourage forward motion in order to avoid walking into the robot's blind spot and increasing the chance of seeing artifacts with the forward facing sensors)
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{\st{Mission Planning} - Remove?}\label{sec:mission_planning}
% %\inst{not clear if we need this section}
% %\todo{Scott-Kyon-FadNik} \ph{Mission and its challenges}

% %In the SubT Challenge, robots are expected to rapidly map, navigate, and search unknown underground environments, and report back to the human supervisor at the base station. Particularly,t

% \st{
% The Urban Circuit of the DARPA SubT Challenge presented many challenges to competitors and their autonomous robots. This included complex topology (multiple stories, dead-ends, junctions, narrow corridors and open spaces), mobility challenges (stairs, vertical shafts), and communication mitigating layouts (non-line-of-sight labyrinth) which can result in intermittent communication between collaborative robots and the base station. Our robots are designed to autonomously detect and explore new frontiers, as well as sharing distinctive environmental elements with each other and the base station. To ensure optimal communication and thus frontier advancement in any given environments, Au-Spot can deploy communication nodes.}

% % \todo{Scott-Amanda-Sung} 
% \ph{\st{Behaviors and Modes}}
% %Hence, t

% \st{There are different high-level behaviors in the mission planning layer. 
% We denote the behavior library as}
% \begin{align}
%     \mathcal{B} = \{B^1,\cdots,B^n \}
% \end{align}
% \st{The role of the mission planner is to generate a sequence of behaviors:}
% \begin{align}
%     (B_0,B_1,\cdots,B_T) = \arg\min \; \textit{MissionRisk} (\mathcal{B}) 
% \end{align}

% \st{A given behavior $B^i$ is executed by (i.e., defined as) visiting a set of anchor points (i.e., nodes) in the information space as explained further below.}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Area coverage and search behavior}
Our mission planning objective %of the DARPA SubT Challenge 
is to coordinate a team of autonomous robots to rapidly map and navigate a large (multi-Km), unknown environment characterized by complex topology and terrain under a one-hour time constraint. The global planner \rev{realizes} this objective by maximizing the area covered by the sensors' footprint along the planned trajectory, within the mission time limit. The planner relies on a representation of the environment with an information-rich graph structure which reduces the policy search space to one that is tractable when exploring large environments over long time horizons.

\ph{Global Planner} We employ a sparse bidirectional graph $G = (V, E)$ that captures the connectivity of the free space in the environment. A node $v_i \in V$ represents a robot pose, and an edge $e_{ij} \in E$ represents a robot transition between connected nodes $v_i$ and $v_j$. Each node $v_i$ has attached to it a feature vector containing the probability $p_{c}(v_i)$ that the robot has seen a sensor-defined neighborhood around the node. Likewise, each edge $e_{ij}$ will induce a local path computed according to Eq.~\ref{eq:policy}. Then, to each edge $e_{ij}$ we attach a feature vector containing the path length $\ell_{e_{ij}}$ and path traversability risk $\mathcal{R}^{e_{ij}}$ computed by Eq. \ref{eq:risk}.

% Likewise, each edge $e_{ij}^g$ has attached to it a feature vector containing the risk $\mathcal{R}^{e_{ij}}$ and edge length $\ell_{e_{ij}}$ %of the optimal grid trajectory between connected nodes $v_i$ and $v_j$, 
% computed according to Eqs.~(\ref{eq:risk}-\ref{eq:policy}), \rev{respectively}. \rev{As a result, the global planner $\lambda$ takes the form of a mapping $v_j = \lambda(v_i)$ that returns the next reachable node on the graph that maximizes the space coverage in the long run.}
% \rev{In the following paragraphs, we briefly discuss steps involved in computing $\lambda$.}

\ph{Graph Construction} 
%As an approximation of $p_{c}(v_i)$, 
We partition graph nodes to frontier nodes $v_f \in V$ and breadcrumb nodes $v_b \in V$. A frontier node $v_f$ indicates the areas of the map that have not been fully explored yet, typically, at the boundary of the known and unknown free spaces. A breadcrumb node $v_b$ indicates the areas of the map that have already been fully explored. As the robot moves in the environment to cover and search the space, the graph is expanded through the addition of new breadcrumb and frontier nodes. \rev{Formally}, visiting a frontier node $v_f$ is expected to lead to changes in the map belief $p(v_f)$, whereas visiting a breadcrumb node will not significantly impact our knowledge of the map coverage.
% Hence, we consider \rev{frontier nodes as known-free areas that need to be covered, while breadcrumb nodes are known-free, covered area that the robot can pass through to reach another frontier node. New breadcrumb and frontier nodes are added to the graph while the robot moves to cover the environment. }

\ph{Graph Policy}
Graph policy $\lambda$ will guide the robot towards the next best node on the graph to maximize the covered area. Specifically, we compute a mapping (feedback policy) $\lambda:V\rightarrow V$ on the graph that maximizes a reward function encoding a trade-off between coverage information and traversal cost. A macro action $\lambda(v_i)$ induces traversal along edge $e_{ij}$ and updates the graph from $V$ to $V^\prime$. In the following, we discuss steps involved in computing $\lambda$.


% when we take a macro action evolve v to v prime

%\rev{\st{Frontiers are generated by simulating a rollout policy that predicts how the graph will grow at $v_f$ by taking multiple actions into the future. Accordingly, $v_f$ is representative of an isolated look-ahead subtree which is rooted at the frontier.}}

\ph{Coverage Information} The \texit{coverage belief} of a graph composed of nodes $\{v_i, \cdots ,v_N\} \in V$ is defined as: 
\begin{align}
    \nonumber
    P_c(V) = \{p_c(v_i), \cdots , p_c(v_N)\}
\end{align}
where $p_c(v_i)$ is the occupancy Bernoulli distribution over a local map centered at node $v_i$. We use entropy to measure the posterior uncertainty of the graph coverage. Entropy of random variable $x\sim p(x)$ is defined as $H_p(x) = E[-\log p(x)]$. Thus, the graph coverage entropy can be represented as:
\begin{align}
    \nonumber
    H_{p_{c}}\!(V) \!=\! -\!\!\!\sum_{v_i \in V}\!\!
    p_{c}(v_i) \log p_{c}(v_i) 
    \!+\!
    \left(1-p_{c}(v_i)) \! \log (1-p_{c}(v_i)\right)
\end{align}

\ph{Coverage Information Gain}
% The coverage reward can be quantified as \emph{information gain}. 
% Information gain in our belief $p_{c}(V)$, i.e., the amount of uncertainty reduction, is associated with
The coverage information gain (i.e., coverage uncertainty reduction) in belief $p_{c}(V)$ induced by macro action $\lambda(v_i)$ is defined as:
\begin{align}
    % I(\lambda(v_i), w_{cov}) &= H_{p_{c}}(V) - H_{p_{c}}(V^\prime; \lambda(v_i), w_{cov}) \\
    I(V \, | \, \lambda(v_i)) &= \underbrace{H_{p_{c}}(V)}_\text{current entropy} - \underbrace{H_{p_{c}}(V^\prime \, | \, \lambda(v_i))}_\text{future entropy}
\end{align}
where the second term represents the expected future entropy of the graph after the agent has executed macro action $\lambda(v_i)$. 
% The successor graph state $p_{c}(V^\prime)$ is computed according to Bayes' theorem. 

% \ph{Action Cost} The cost associated with an action $\lambda(v_i)$ is defined as:
% \begin{align}
%     C(E \, | \, \lambda(v_i)) = \ell_{e_{ij}}
% \end{align}
% where $\ell_{e_{ij}}$ is the path length of the trajectory specified by grid policy $\pi_{grid}^*$, Eq.~(\ref{eq:decoupled}).

\ph{Generalized Reward} To capture energy and distance measures in the coverage planning, we define the one-step reward to be the weighted sum of information gain and distance traveled under the macro action $\lambda(v_i)$:
\begin{align}
    % R(v_i, \lambda(v_i)) &= \alpha_I I(V; \lambda(v_i)) - \alpha_C L
    Rew(v_i \, , \, \lambda(v_i)) &= w_1 \; I(V \, | \, \lambda(v_i)) - w_2 \; \ell_{e_{ij}}
\end{align}
where $w_1$ and $w_2$ weigh the information gain and traveled distance, respectively. %We aim to minimize traversal cost $\ell_{e_{ij}}$ as it is indicative of the robot actuator output required to execute macro action $\lambda(v_i)$.  

% \ph{Solving}
% Let $v(k)$ denote the $k$-th node visited under graph policy $\lambda$. Similarly, $e(k,k+1)$ denote the edge between nodes $v(k)$ and $v(k+1)$.
% %We let $k, k+1, ... ,K$ indicate the global planning time-steps. 
% \rev{We solve for the mapping from $V$ to $V$  Then we find the sequence of global planning actions that maximize the sum of future expected reward:}
% \begin{align}
%     \lambda^* &= \arg\max_\lambda \, \mathbb{E} \left[ \sum_{k'=k}^{K} Rew(v(k') \, , \, \lambda(v(k'))) \right] \\
%      \nonumber
%      ~s.t.,~~~ & \mathcal{R}^{e(k',k'+1)} < \psi(t),~~~\forall k' \in \{k, ... , K-1\}
% \end{align}
% where $\psi(t)$ is a time-varying acceptable risk threshold. As the duration of the mission progresses, the risk threshold \rev{$\psi(t)$} can increase to allow for riskier actions.


\ph{Graph Policy Optimization}
Let $v(k)$ denote the $k$-th node visited under graph policy $\lambda$. Similarly, let $e(k,k+1)$ denote the edge between nodes $v(k)$ and $v(k+1)$.
%We let $k, k+1, ... ,K$ indicate the global planning time-steps. 
We solve for the mapping $\lambda$ that maximizes the sum of future expected reward:
\begin{align}
    \lambda^* &= \arg\max_{\lambda(\cdot)} \, \mathbb{E} [ \sum_k Rew(v(k) \, , \, \lambda(v(k))) ]
\end{align}
This optimization can be solved via value iteration-based methods or forward search methods.
%where $\psi(t)$ is a time-varying acceptable risk threshold. As the duration of the mission progresses, the risk threshold \rev{$\psi(t)$} can increase to allow for riskier actions.

% \ph{Global Planning Solver}

% \ph{Unified Policy: Global Policy + Perception-Aware}




% We refer to this graph as the IRM (information roadmap) as its nodes ($V$) and edges ($E$) are enriched by various environmental and robot-related properties. As an example, in the Urban Circuit, a node at the entrance of a staircase is marked with a special attribute which indicates that a high level of traversability%, unique to Spot, 
% is required to plan paths through this node. 

% \ph{Information Roadmap}
% IRM nodes are categorized as either breadcrumbs ($\beta\in\mathbb{B}$), frontiers ($f\in F$), or the home base ($h\in H$) where the mission is initiated. 
% \begin{align}
%     V = (F, \mathbb{B}, H)
% \end{align}
% Frontier nodes are located at the boundary between unexplored and known space, and are connected to breadcrumb nodes though collision-free edges. %9
% Pose graph-fixed breadcrumb nodes trace the history of the robot's motions and, hence, indicate both known areas and traversable paths via breadcrumb edge connections. %4.5
% Furthermore, since robots share their pose graph with one another, frontier nodes are never placed in already explored areas.  
% %Also, frontier nodes and their edge connections on the IRM encode where the unexplored regions are, how significant they are for the global coverage, and how to traverse to them in this multi-level environment.
% %The global planning module provides services to maintain and query the IRM, such as adding/pruning frontier nodes. Behaviors can query the global planner for the shortest path to frontiers, closest stairs, and the base station. Additionally, the planner is responsible for ensuring that the locations of the IRM nodes remains consistent after a pose-graph relaxation from the global localization module.  

% % \begin{align}
% %     E = (T, D, ...)
% % \end{align}

% %\todo{Amanda-David-Sung}
% \ph{Exploration Planning} 
% The objective of the global planning module is to assign frontiers as terminal goals to maximize global coverage of the environment by multiple robots, including the Au-Spots, for the allotted time. Frontier goals are assigned to the robot according to either a local or global selection approach. 
% \subsection{Local Frontier Selection}
% Given the robot's current pose ($x_k$), we select a frontier goal ($f_g$) with minimum bearing (i.e. relative planar angle between robot's forward direction and frontier location), subject to traversability and bearing constraints:
% \begin{align}
%     J^{local}(V, E, x_k) &= \theta_{bearing}(x_k, f) 
% \end{align}
% \begin{align}
%     f_g &= \arg\min_{f\in V} J^{local}(V, E, x_k)\\
%     ~s.t.,~~~ & P_{travs}(x_k, f) > \gamma \nonumber \\
%     % & f \in Disc(x_k, r)\\ NOTE we choose this range so we can make a judgement about traversability, so prob not necessary to make it a separate constraint
%     & |\theta_{bearing}(x_k, f)| < \pi/2 \nonumber
% \end{align}
% $P_{travs}(x_k, f)$ is the probability of a traversal between $x_k$ and $f$. 
% While the first constraint ensures that the selected frontier is traversable, the latter constraint on the bearing angle $\theta_{bearing}(x_k, f)$ results in a depth-first exploration behavior. 
% Note that for the local frontier selection, the frontiers in the direction of robot's current heading are preferred in order to minimize velocity changes during traversal. This scheme is particularly helpful for Au-Spot so that it can walk continuously at higher speeds and minimize turning motions which may cause risky foot placements at the blind spots (at the rear-left and -right) of the robot.

% \subsection{Global Frontier Selection}
% Global frontier selection is initiated if no frontier exists satisfying local selection constraints. Frontiers $F$ in the IRM $G$ are clustered according to the minimum number of edges connecting any two sets of observations through a hierarchical agglomerative clustering strategy. A cluster is defined as:
% \begin{align}
%     % cluster(f) = \{ f' | d(f', f) < n_{E} \}\in\mathcal{C}
%     C(f) = \{ f' | dist(f', f) < n_{E} \}\in\mathcal{C} 
% \end{align}
% Since cluster size is indicative of an unexplored region's expanse, we use it to predict expected information gain, or the utility of traversing to a frontier in that cluster. For a given cluster, the minimum number of edges between the robot's current position and the cluster is the cost of traversal. We select a frontier goal $f_g$ closest to the centroid of the cluster $C_g(f)$ which maximizes our cost-utility function:
% \begin{align}
%     J^{global}(V, E, x_k) = w_{d}(dist(x_k, f))^{-1} + w_{s}|C(f)|
% \end{align}
% \begin{align}
%     C_g(f) &= \arg\max_{C\in \mathcal{C}} J^{global}(V, E, x_k) \\
%     % ~s.t.,~~~ & dist(x_k, C) < \gamma' \nonumber \\
%     f_g &= \arg\min_{f\in C_g(f)} dist(f, centroid(C_g(f)))
% \end{align}
% where $w_d$ and $w_s$ weigh the cost of traversal and new information gain, respectively. 
% A robot travels to a global frontier goal $f_g$ by backtracking and planning paths along its breadcrumbs.
% Since the robots share their explored regions with each other, the global planner can automatically deploy them to different sectors of the environment. 
% \begin{align}
%     J(V, E; x_k) = dAng(f, x_k)
% \end{align}

% \begin{align}
%     nextWyapoint &= \arg\min_{f\in V} J(V, E)\\
%     ~s.t.,~~~ & P(f, x_k) > \gamma \\
%     & f \in Disc(x_k, r)\\
%     & dAng(f, x_k) < \zeta
% \end{align}
% \begin{align}
%     cluster(f) = \{ f' | d(f', f) < r \}\in\mathcal{C}
% \end{align}


% \begin{align}
%     J^{global}(V, E; x_k) = w_s |Cluster(f)| + w_d degree(f)
% \end{align}

% \begin{align}
%     C^s &= \arg\max_{c\in \mathcal{C}} J^{global}(V, E)\\
%     ~s.t.,~~~ & d(C, f) < \gamma' \\
% \end{align}

% \begin{align}
%     f^* &= centroid(C^s)
% \end{align}


% \begin{figure}[thpb]
%   \centering
%   \includegraphics[width=0.40\textwidth]{graphics/spot_planning.PNG}
%   \caption{\inst{not sure if we need this} Planner Architecture on Spot. TODO@Nikhilesh: Make cleaner graphic}
%   \label{figurelabel}
% \end{figure}

% Then, we need to discuss how traversability and risk values (from the previous section) appear on IRM edges... \inst{since we are not doing this yet, this might need some coding or we can remove this section}... similarly, we should show how stairs semantics appear on teh IRM... similarly, we can discuss when the robot calls operator for help (it shoudl be the "result" of planning. for example, when the planning returns a plan which has a risk beyond 70 percent). The output of planning could be from a activity library (e.g., explore, rtb, rtc, teleop, stair climbing,...). This set (activity library) needs to be defined mathematically and the plannign probelm shoudl be formulated as a optimization over this activity library space.


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.4\textwidth]{spot_iros/graphics/spot_eagle_rock_shrinked_black_1.png}
  \caption{\rev{Map created by Au-Spot exploration of Eagle Rock Substation, Los Angeles, with different odometry sources: proposed method in green against a) KO and b) KVO.}}
  \label{spot_eagle_rock}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.4\textwidth]{spot_iros/graphics/spot_proposed_office.PNG}
  \caption{\rev{Map created by Au-Spot while exploring an office building at NASA's JPL with different odometry sources: a) KO b) KVO c) Proposed}}
  \label{spot_indoor_office}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\todo{everyone}\inst{combine below two paragraphs}
The NeBula autonomy architecture is implemented on two Boston Dynamics Spot robots and  field-tested in subsurface, multi-level, and perceptually-degraded GPS-denied environments, including underground unstructured environments and industrial power plants.

As part of the Urban Circuit of the DARPA Subterranean Challenge, two Au-Spots were deployed into an Industrial Plant in February 2020 for live-mission exploration and  3D-Mapping of the environment (Fig. \ref{fig:alpha_course_stairs_map}).  The missions included detection of artifacts of interest such as backpacks, human survivors, gas-leaks, and cellphone via different sensing modalities including vision, thermal, gas sensors, wifi detection, among others. The competition divided into four live-missions in unknown environments. %between two different courses (two Alpha course attempts and two Beta course attempts) where their best score for each course was tallied and counted towards their final score. 
Our Au-Spots explored a combined distance of 4km, including successfully climbing multiple flights of stairs. Points were awarded for accurate artifact detection and for successfully reporting this information back to the base-station.   
% TODO July 29: Describe there are four runs --Thomas: DONE
%The Au-Spots were able to explore \rev{the plant successfully and accurately report artifact locations back to the human-supervised base station, in real-time.}
%multi-level environments and report back accurate artifact locations to the human supervisor at the base station.
The Au-Spots successfully detected and localized a total of 16 artifacts, capturing 16 points \rev{and thus propelling our team CoSTAR to a 1st-place victory. }
\todo{A video highlighting Au-Spots in the challenge is available at [X].}

%25 artifacts across all four runs with the best scores from each course amounting to 16 points which propelled our team CoSTAR to a 1st-place victory. The Au-Spots explored a combined distance of 4km, including successfully climbing multiple flights of stairs, in all live-missions.
%Specifically, the two traveled a combined distance of 4 km in four runs, including four successful stair climbing of multiple flights of stairs. 

%mentioning the sensors is kind of redundant, we have mentioned that that is how they do their detection earlier in the paper and we know that Au-Spot is NeBula enabled. 










\ph{Odometry Estimation}
To demonstrate the performance of the proposed odometry pipeline, we compare the localization accuracy achievable using individual sensing channels with the proposed uncertainty-aware multi-sensor approach in perceptually-degraded environments. Figure \ref{spot_eagle_rock} depicts the results of the proposed method on data collected in the Eagle Rock subway station, Los Angeles. The rough and \rev{varied} terrain causes KO to perform poorly, while a large amount of visual features causes KVO to produce a sufficiently accurate map. Conversely, Fig. \ref{spot_indoor_office} depicts results from data collected in NASA JPL's 198/161 offices. In this case, the soft carpet on the floor results in KO providing much more accurate maps than KVO.  KVO is instead challenged by feature-less white walls and the repetitive visual textures of the carpet. Features of various courses make perception and odometry estimation challenging to \rev{varied} sensing channels, hence the need for multi-channel odometry fusion. As seen in both figures, the proposed odometry generation method results in more accurate maps than those obtained by KO or KVO-based odometry.

% -------------------------------------------------------------------

% Eagle Rock: A) Hard concrete surface (bad KO), B) Visually-rich environment (good VO), C) HeRO detects jumps in KO and chooses VO

% 198 Office: A) Soft moquette surface (good KO), B) Texture-repetitive moquette and featureless walls (bad VO), C) HeRO detects jumps in VO and chooses KO

% -------------------------------------------------------------------

%To demonstrate the performance of the proposed odometry pipeline on a legged robot, we evaluate and compare the localization accuracy using individual sensing channels with the proposed uncertainty-aware multi-sensor approach in perceptually-degraded environments. Figure \ref{spot_eagle_rock} depicts the results of the proposed method on data collected in an underground subway station in Eagle Rock, Los Angeles. As seen in this figure, KO performs poorly in this environment whereas KVO results in a sufficiently accurate map. Conversely, Fig. \ref{spot_indoor_office} depicts the results from the data collected in NASA JPL's 198/161 buildings where KO provides much more accurate maps than KVO. Features of various courses makes the perception and odometry challenging to various sensing channels, hence the need for multi-channel odometry fusion. As seen in Fig. \ref{spot_eagle_rock} and Fig. \ref{spot_indoor_office}, the proposed odometry generation method results in consistently more accurate maps than those obtainable by KO or KVO-based odometry. 









%Finally, the proposed odometry solution was employed by team CoSTAR's Au-Spots at the DARPA Subt Challenge and this resulted in highly accurate maps.

%Finally, the proposed odometry solution has been employed on two Spot robots of the CoSTAR team during the exploration of a dismissed multi-level power plant in Elma, WA at the Urban Circuit of the DARPA SubT Challenge, resulting in highly accurate maps. % and leading to the first place for the CoSTAR team in this challenge. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.35\textwidth]{spot_iros/graphics/coverage_matteo.pdf}  % .pdf one is with scale
  %\caption{Area explored by a fleet of two Spots and two wheeled robots.  Colored pointclouds indicate explored areas, with colors indicating z height, spanning 3 floors. (Image credit: DARPA).}
  \caption{\rev{Area explored in the SubT Beta Course by a team of two UGV's and one Au-Spot. Colors indicate robot-specific coverage (Red: UGV 1, Blue: UGV 2, Yellow: Au-Spot 1).}}
  \label{fig:exploration_planner_topview}
\end{figure}

\begin{figure}[t!]  % Moved from odometry section; Revert if necessary
  \centering
  \includegraphics[width=0.35\textwidth]{graphics/spot_alpha_course_stairs.PNG}
  \caption{\rev{Map produced with the proposed method in the SubT Alpha Course.}}
  \label{fig:alpha_course_stairs_map}
\end{figure}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Planning and Autonomous Exploration}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ph{Traversability}
% The traversability analysis module creates risk maps of size 20mx20m using sensory information obtained from spatial fusion of LIDAR and odometry data.
%For this, traversability analysis is performed on the map coming from a spatial fusion of LiDAR data with odometry \cite{oleynikova2017voxblox}.
%Having a risk map this big is crucial for bridging the gap between Spot's internal planner, which only keeps a robot--centric traversability map in the size of 4x4m, and the goals that are given by the global planner, which can be up to 10 m away from the robot's current position. %Besides long-range planning in this map, the 
Our perception-aware local planner enabled Au-Spot to safely and efficiently navigate through difficult environments, which contained a variety of unstructured obstacles and terrain, including ramps, slopes, piles of twisted metal, machinery and rubble.  Negative obstacles such as stairs and random voids were successfully avoided. {\color{red} missing data - if we cannot add experiments, we should remove this paragraph.}
% During the mission Au-Spot also deployed comm nodes in order to enable Au-Spot to explore safely and to avoid planning paths which may potentially fall into Spot's in-built sensors' blind spots.  %considers metrics such as negative obstacles, sensor blind spots, or dropped comm nodes, to enable Au-Spots to avoid negative obstacles, such as stairs, random voids going down or avoid planning paths through the in-built sensors' blind spots.

\ph{Coverage Planner}
Au-SPOT coverage planner in the DARPA challenge successfully enabled a fleet of two Au-Spots and two wheeled UGV robots to collaboratively explore and map the environment, covering unexplored territory within the one hour time limit. Fig. \ref{fig:exploration_planner_topview} depicts the area explored by the robots during one of the four live-mission runs. In this single (one hour) run, the collaboration of robots mapped a total volume of \rev{(roughly)} 25000 ${m}^{3}$. %distance of 1.3km. 
One of the most challenging elements of the course was traversing 4 flights of stairs, which induces pitching motion, physical slippages on the stair edges, and poor visual cues due to repetitive patterns of the stairs and railings. Fig. \ref{fig:stairs-firstPage} shows Au-Spot successfully climbing down stairs and Fig. \ref{fig:alpha_course_stairs_map} depicts the map produced during the stair climbing operations.  Due to the high-precision localization ($\leq$ 5 m error), Au-Spot was able to accurately localize artifacts on multiple levels of the industrial power plant.



% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.45\textwidth]{spot_iros/graphics/coverage_matteo.png}
%   %\caption{Area explored by a fleet of two Spots and two wheeled robots.  Colored pointclouds indicate explored areas, with colors indicating z height, spanning 3 floors. (Image credit: DARPA).}
%   \caption{\rev{Area explored in the SubT Beta course by a fleet of two UGV's and one Au-Spot. Colors indicate robot-specific coverage (Red: UGV 1, Blue: UGV 2, Yellow: Au-Spot 1).}}
%   \label{fig:exploration_planner_topview}
% \end{figure}

% \begin{figure}[t!]  % Moved from odometry section; Revert if necessary
%   \centering
%   \includegraphics[width=0.45\textwidth]{graphics/spot_alpha_course_stairs.PNG}
%   \caption{Map produced with the proposed method in \rev{the SubT} Alpha Course of DARPA SubT Challenge Urban Circuit}
%   \label{fig:alpha_course_stairs_map}
% \end{figure} % Fadhil moved this above but cannot move the comment


%\textbf{Multi--Level}
%The fully autonomous exploration behaviour, combined with the semi--autonomous stair climbing capability, enabled the Spots to explore multiple levels of the testing facility.
%Fig. \ref{fig:alpha_course_stairs_map} shows the reconstructed map from a multi--level exploration mission.


%\begin{figure}[thpb]
%  \centering
%  \includegraphics[width=0.47\textwidth]{example-image-a}
%  \caption{Exploration on Multiple Levels.}
%  \label{fig:exploration_planner_multilevel}
%\end{figure}


% \begin{figure}[thpb]
%   \centering
%     \begin{subfigure}[t]{0.5\linewidth}
%         \centering
%         \includegraphics[height=1in]{example-image-a}
%         \caption{Detecting Positive Obstacles}
%         \label{fig:trav_costmap_pos_obs_wall}
%     \end{subfigure}%
%     ~ 
%     \begin{subfigure}[t]{0.5\linewidth}
%         \centering
%         \includegraphics[height=1in]{example-image-b}
%         \caption{Detecting Negative Obstacles (down stairs)}
%         \label{fig:trav_costmap_neg_obs_stairs}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.5\linewidth}
%         \centering
%         \includegraphics[height=1in]{example-image-a}
%         \caption{Avoiding dropped comm nodes}
%         \label{fig:trav_costmap_comm_nodes}
%     \end{subfigure}%
%     ~ 
%         \begin{subfigure}[t]{0.5\linewidth}
%         \centering
%         \includegraphics[height=1in]{example-image-a}
%         \caption{Avoiding Sensor Blind Spots}
%         \label{trav_costmapb_bd_blindspot}
%     \end{subfigure}%
%     \caption{Traversability Costmaps}
% \end{figure}

% \ph{Local Planning}
% Our local planner uses our bigger traversability map to plan paths to the goals given by the global planner.
% As already mentioned, this helped to overcome sub--optimal paths, due to longer planning horizon, and see obstacles, which are not visible to the internal planner.
% Additionally, the perception-aware maneuvers as described in Section \ref{sec:local_planning}, helped to bias the robot to move forward.
% Because the internal sensors have a blind spot in the rear--left and rear--right and the FoV is better in the front, this succesfully helped to avoid cases, where the robot would step into its blind spots.

% Figure ??? shows the output of the local planner, consisting of the costmap, the A* path and the selected waypoint with the heading, which biases the robot to move forward.
% \begin{figure}[thpb]
%   \centering
%   \includegraphics[width=0.47\textwidth]{example-image-a}
%   \caption{Long--Range Costmap + Dijkstrapath + Waypoint}
%   \label{fig:local_planner}
% \end{figure}

% \begin{figure}[thpb]
%   \centering
%   \includegraphics[width=0.4\textwidth]{graphics/irm.png}
%   \caption{IRM constructed while exploring a single level.}
%   \label{figurelabel}
% \end{figure}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \ph{Stair Climbing and Multi-level Mapping}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \rev{Au-Spot successfully climbed multiple flights of stairs during the Urban Circuit of the DARPA Subterranean Challenge. The encountered staircases are among more challenging cases, where the stairs are metal-grated and approximately 0.27m in run, and 0.19m in rise. 
% Each staircase consists of four flights of stairs with tight 180-degree turns at each landing.}
% Fig. \ref{fig:stairs-firstPage} shows a snapshot of \rev{Au-Spot in successfully climbing down stairs.} 
% Stair climbing operations pose challenges to state estimation due to the frequent pitching motion, physical slippages on the stair edges, and repetitive patterns of the railing. 
% Fig. \ref{fig:alpha_course_stairs_map} is the map produced during the stair climbing operations, which led to high-precision localization of artifacts ($\leq$ 5 m error).

% \ph{Stair Climbing}
% A special case that is not handled by the local planner is stair climbing. 
% %\rev{Stairs will either appear as positive or negative obstacles in the risk map.} 
% In the current setting, stair climbing is semi-autonomous. The human supervisor has to \rev {\textit(i)} detect the presence of a stair case \rev{(e.g. from the robot Camera/LIDAR feed)}, \rev {\textit(ii)} switch Spot's internal planner to 'Stair' mode which configures the planner for climbing stairs, and then \rev {\textit(iii)} give \rev{a goal} at the end of the stair case. The actual climbing is then completely handled by Spot's internal planner and controller.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.45\textwidth]{graphics/spot_stairclimbing2.jpg}
%   \caption{Spot stair climbing in Urban Circuit Alpha course.}
%   \label{fig:spot_stair_climb}
% \end{figure}

% \begin{figure}[thpb]
%   \centering
%   \includegraphics[width=0.47\textwidth]{spot_iros/graphics/spot2_alpha2_ape.png}
%   \caption{\rev{Absolute Position Error (APE) of Au-Spot 2 in Alpha 2 course. TODO: To be linked with text}}
%   \label{spot2_alpha2_ape}
% \end{figure}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Spot Operation in DARPA SubT Urban Circuit}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \inst{Lots of amazing pictures, accuracy of localization, traversability
% Images of: Spot in extreme terrain, LAMP Map, IRM, Comms map, Comm dropping, Stair Climbing}

% \newpage

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Lessons Learned and Future 
% Directions}\label{sec:lessonslearned}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Robosimian's journal intro sentences of lessons learned
% This section summarizes some of the highlights of our platform,
% discusses the lessons learned, and justifies the core
% assumptions we made in design, testing, and operations:
% % I think its not in order yet

% % Fadhil. May be this subsection can be moved to Robot behaviors?
% \subsection{Recovery}
% Due to the challenging terrain of Spot at the test area, we consider a possibility that the robot might fall.
% %
% Fortunately, Spot has ability to "SELFRIGHT" itself from falling to sitting position and still operational after that.
% %
% However, on some cases, we find that the robot is not able to stand up after a successful "SELFRIGHT" because the legs placement are not stable after the recovery.
% %
% As a work around to robustify the behaviors in the current Spot software that we use, we send a repetitive command of quick stand and sit to improve better placement of the legs.

% \subsection{Traversability}
% Currently, traversability is purely defined geometrically (obstacles, slopes, holes,...).
% However, when traversing challenging terrains, as encountered in the SubT challenge, the notion of traversability has to be extended.
% While there are many metrics, which can be considered for traversability and that have been described by Papadakis in \cite{Papadakis2013}, the most important one would be surface friction.
% Due to small contact surface and, compared to wheeled or tracked robots, and dynamic gaits, quadrupeds suffer more from slippery surfaces such as wet or dusty surfaces. 

% For this reason, traversability has to include surface friction informations so that it can be avoided by the local planner, if possible.
% If avoidance should not be possible, this information could at least be used to adaptively set the surface friction coefficient, which is used by the Spot's internal planner.

% Another traversability metric might include terramechanics in order to avoid soft soil, where the feet of the robot might sink in.

% Another traversability metric might be to not step on movable items (loose plates or things on which Spot crashed)

% => We need traversability from semantic scene understanding, not just geometric anymore!!!

% \inst{This kind of discussion should go to the experiment section. - This does not fit the method section} The Nebula global planner, which is described in more detail in Section \ref{sec:mission_planning}, can select goals up to 10 m away from the current robot's position.
% However, Spot's internal planner works with a 4x4 m map of the environment.
% Even though, waypoints outside of the internal map can be given to the internal planner, this caused some issues such as the robot taking sub--optimal paths or not avoiding some obstacles such as overhangs, cliffs or obstacles within the blind spots of the internal sensors.
% Because, the Nebula payload offers more sensing capabilities, especially due to the long--range sensors such as the LiDAR, we chose to add an intermediate planner in order to bridge the gap and avoid and complement the Spot internal planner.

% ======

% \subsection{Perception--Aware Planning}
% Legged robots, such as Spot, have a higher degree--of--freedom than most ground robots, which usually have a fixed chassis.
% This higher level of configurability decouples the robot's overall motion from the floating base's attitude to a certain extent.
% One interesting direction would be to exploit this characteristic of the floating base for perception--aware planning.
% For example by pitching or rolling the robot's base in order to maximize sensor coverage and reduce blind spots.

% ====

% We have access to BD costmap and additional obstacle information, which spot might not see. When giving global waypoints (far away), the robot might not be able to avoid some of these or end up taking suboptimal paths due to the small costmap (4x4m). We plan intermediate waypoints (with Dijkstra) to avoid this problem. Additionally, we prefer forward motion due to sensor coverage (blind spot avoidance and artifact detection), so we choose the waypoint heading in a way which encourages the robot to go forward. Waypoints are given to Spot's internal planner, which then executes the actual trajectory.

% Mentio that, even though we can try to avoid these regions by not giving waypoints in that region we cannot guarantee that BD will actually avoid these regions.

% Define, how we assign an orientation to the previously selected waypoint in order to bias forward motion


% \subsection{Intelligent Behaviour Selection}
% Automatics stair detection and climbing, changing gaits (crawl) to get over rubble,...)
% BD offers a very limited, but powerful, set of tuning knobs (gait, stair mode, surface friction, body height,...).
% However, these are either set to fixed average values or chosen by the human supervisor.
% One push would be to switch and set these tuning knobs autonomously (for example choose gait based on what the long range sensors see, set surface friction by estimating surface friction, autonomously change to stair mode and do autonomous stair climbing instead of human given wp)


% \subsection{Collaboration}
% Spot as drone carrier?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}

This system-focused paper reports our effort on endowing the Boston Dynamics Spot robot with high-levels of autonomy, hardware and perception capabilities that won the Urban Circuit of the DARPA Subterranean Challenge.
%This system-focused paper reports our work on endowing high-level autonomy and perception capabilities on the Boston Dynamics Spot robot that won the Urban Circuit of the DARPA Subterranean (SubT) Challenge. 
In this paper, we discussed our NeBula autonomy architecture which we augmented to Boston Dynamics' Spot and described specific aspects in enabling legged autonomy for complex missions.  
% and explained in more detail the odometry and risk-aware planning aspects of the autonomy framework. 
We believe this work is an important step in demonstrating the capabilities of legged robots for accomplishing complex, real-world live-missions in extreme environments.

%Motivated by exploring extreme environments and in particular underground environments in DARPA SubTerranean Challenge, this paper pushes the boundaries of the state-of-practice in enabling legged robotic systems to accomplish real-world complex missions in relevant scenarios. In particular, we will discuss the behaviors and capabilities which emerge from the integration of the autonomy framework NeBula (Networked Belief-aware Perceptual Autonomy) with next generation mobility systems. We will discuss the design, hardware, software challenges and solutions in mobility, perception, autonomy, and wireless networking, as well as lessons learned and future directions. We demonstrate the performance of the proposed system and solutions on physical systems in real-world scenarios, which achieved first place in the 2020 DARPA Subterranean Challenge Urban Circuit.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}
% A video of the experimental results is available at
% \url{https://youtu.be/_HpWIhFFD54} . \inst{Too dramatic to put on a paper?. Should we put it below abstract?}


% \section*{ACKNOWLEDGMENT}
% The research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. This work was partially funded by the Defense Advanced Research Projects Agency (DARPA). Government Sponsorship acknowledged.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{main}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}































%------------Review Highlights ---------------------

% Before GENERAL, then ODOMETRY-ORIENTED 

% Environment-INDUCED and Motion-INDUCED Challenges

% 1- One of the main focuses of this work is to enable autonomy in perceptually-degraded environments.

% 2 - This includes ... gps-denied, dark, obscurantc.... This elements pose significant difficulties on various ?sensing modalities?

% 3 - Further, strong platform vibration originated by mobility-stressing terrains exacerbates sensor measurments quality while adding additional uncertainty on sensor measurements (and introduces additional uncertainty on sensor measurements).

% 4 - A reliable odometry source in perceptually-degraded settings is a challenge. %4 --- TODO: Make Better

% A reliable odometry source in perceptually-degraded settings is a challenge. %4 --- TODO: Make Better -  % Motion-induced Challenges

% To overcome these challenges, NeBula characterizes uncertainty and failure modes associated with each sensing modality and formulates a 

% To address these challenges, NeBula relies on a multi-sensor solution to enable odometry in perceptually-degraded setting. Characterizing uncertainty associated and failure modes associated with each sensing modality, NeBula and fuses various sensing modalities in an uncertainty-aware manner

% with health monitor and switch-on-failure capability, that at every time step $k$, selects the odometry channel with the most reliable output transformation $T_k \in SE(3)$.

% motion-prior OR prior INSTEAD of external - Just calling it Y - OK + OR T_init

%---------------------------------------------------------